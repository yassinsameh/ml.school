[
  {
    "objectID": "studio-setup.html",
    "href": "studio-setup.html",
    "title": "Setting up SageMaker Studio",
    "section": "",
    "text": "import os\nimport sys\nfrom pathlib import Path\n\nCODE_FOLDER = Path(\"code\")\nCODE_FOLDER.mkdir(parents=True, exist_ok=True)\n\nsys.path.append(f\"./{CODE_FOLDER}\")\n\nDOMAIN_ID=os.environ[\"DOMAIN_ID\"]\nUSER_PROFILE=os.environ[\"USER_PROFILE\"]\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nThe dotenv extension is already loaded. To reload it, use:\n  %reload_ext dotenv"
  },
  {
    "objectID": "studio-setup.html#step-1---customize-kernel-libraries",
    "href": "studio-setup.html#step-1---customize-kernel-libraries",
    "title": "Setting up SageMaker Studio",
    "section": "Step 1 - Customize Kernel Libraries",
    "text": "Step 1 - Customize Kernel Libraries\nYou can customize SageMaker Studio using Lifecycle configurations. These are shell scripts that will be triggered by lifecycle events, such as starting a new Studio notebook.\nThe following script upgrades the packages on a SageMaker Studio Kernel Application.\n\n#!/bin/bash\n# This script upgrades the packages on a SageMaker \n# Studio Kernel Application.\n\nset -eux\n\npip install -q --upgrade pip\npip install -q --upgrade awscli boto3\npip install -q --upgrade scikit-learn==1.3.1\npip install -q --upgrade PyYAML==6.0\npip install -q --upgrade sagemaker\npip install -q --upgrade ipytest\n\nOverwriting code/packages.sh\n\n\nWe can now create a new lifecycle configuration.\n\nDOMAIN_ID=$(echo \"$1\")\nUSER_PROFILE=$(echo \"$2\")\n\nLCC_CONTENT=`openssl base64 -A -in $3/packages.sh`\n\naws sagemaker delete-studio-lifecycle-config \\\n    --studio-lifecycle-config-name ml-school\n\nresponse=$(aws sagemaker create-studio-lifecycle-config \\\n    --studio-lifecycle-config-name ml-school \\\n    --studio-lifecycle-config-content $LCC_CONTENT \\\n    --studio-lifecycle-config-app-type KernelGateway) \n\narn=$(echo \"${response}\" | python3 -c \"import sys, json; print(json.load(sys.stdin)['StudioLifecycleConfigArn'])\")\necho \"${arn}\"\n\naws sagemaker update-user-profile --domain-id $DOMAIN_ID \\\n    --user-profile-name $USER_PROFILE \\\n    --user-settings '{\n        \"KernelGatewayAppSettings\": {\n            \"LifecycleConfigArns\": [\"'$arn'\"]\n        }\n    }'\n\narn:aws:sagemaker:us-east-1:325223348818:studio-lifecycle-config/ml-school\n{\n    \"UserProfileArn\": \"arn:aws:sagemaker:us-east-1:325223348818:user-profile/d-givocgtibv1g/default-1682182522641\"\n}"
  },
  {
    "objectID": "studio-setup.html#step-2---set-up-auto-shutdown",
    "href": "studio-setup.html#step-2---set-up-auto-shutdown",
    "title": "Setting up SageMaker Studio",
    "section": "Step 2 - Set up Auto-Shutdown",
    "text": "Step 2 - Set up Auto-Shutdown\nThe following script configures auto-shutdown of inactive kernels.\n\n#!/bin/bash\n# This script installs the idle notebook auto-checker server extension to SageMaker Studio\n# The original extension has a lab extension part where users can set the idle timeout via a Jupyter Lab widget.\n# In this version the script installs the server side of the extension only. The idle timeout\n# can be set via a command-line script which will be also created by this create and places into the\n# user's home folder\n#\n# Installing the server side extension does not require Internet connection (as all the dependencies are stored in the\n# install tarball) and can be done via VPCOnly mode.\n\nset -eux\n\n# timeout in minutes\nexport TIMEOUT_IN_MINS=60\n\n# Should already be running in user home directory, but just to check:\ncd /home/sagemaker-user\n\n# By working in a directory starting with \".\", we won't clutter up users' Jupyter file tree views\nmkdir -p .auto-shutdown\n\n# Create the command-line script for setting the idle timeout\ncat &gt; .auto-shutdown/set-time-interval.sh &lt;&lt; EOF\n#!/opt/conda/bin/python\nimport json\nimport requests\nTIMEOUT=${TIMEOUT_IN_MINS}\nsession = requests.Session()\n# Getting the xsrf token first from Jupyter Server\nresponse = session.get(\"http://localhost:8888/jupyter/default/tree\")\n# calls the idle_checker extension's interface to set the timeout value\nresponse = session.post(\"http://localhost:8888/jupyter/default/sagemaker-studio-autoshutdown/idle_checker\",\n            json={\"idle_time\": TIMEOUT, \"keep_terminals\": False},\n            params={\"_xsrf\": response.headers['Set-Cookie'].split(\";\")[0].split(\"=\")[1]})\nif response.status_code == 200:\n    print(\"Succeeded, idle timeout set to {} minutes\".format(TIMEOUT))\nelse:\n    print(\"Error!\")\n    print(response.status_code)\nEOF\nchmod +x .auto-shutdown/set-time-interval.sh\n\n# \"wget\" is not part of the base Jupyter Server image, you need to install it first if needed to download the tarball\nsudo yum install -y wget\n# You can download the tarball from GitHub or alternatively, if you're using VPCOnly mode, you can host on S3\nwget -O .auto-shutdown/extension.tar.gz https://github.com/aws-samples/sagemaker-studio-auto-shutdown-extension/raw/main/sagemaker_studio_autoshutdown-0.1.5.tar.gz\n\n# Or instead, could serve the tarball from an S3 bucket in which case \"wget\" would not be needed:\n# aws s3 --endpoint-url [S3 Interface Endpoint] cp s3://[tarball location] .auto-shutdown/extension.tar.gz\n\n# Installs the extension\ncd .auto-shutdown\ntar xzf extension.tar.gz\ncd sagemaker_studio_autoshutdown-0.1.5\n\n# Activate studio environment just for installing extension\nexport AWS_SAGEMAKER_JUPYTERSERVER_IMAGE=\"${AWS_SAGEMAKER_JUPYTERSERVER_IMAGE:-'jupyter-server'}\"\nif [ \"$AWS_SAGEMAKER_JUPYTERSERVER_IMAGE\" = \"jupyter-server-3\" ] ; then\n    eval \"$(conda shell.bash hook)\"\n    conda activate studio\nfi;\npip install --no-dependencies --no-build-isolation -e .\njupyter serverextension enable --py sagemaker_studio_autoshutdown\nif [ \"$AWS_SAGEMAKER_JUPYTERSERVER_IMAGE\" = \"jupyter-server-3\" ] ; then\n    conda deactivate\nfi;\n\n# Restarts the jupyter server\nnohup supervisorctl -c /etc/supervisor/conf.d/supervisord.conf restart jupyterlabserver\n\n# Waiting for 30 seconds to make sure the Jupyter Server is up and running\nsleep 30\n\n# Calling the script to set the idle-timeout and active the extension\n/home/sagemaker-user/.auto-shutdown/set-time-interval.sh\n\nOverwriting autoshutdown.sh\n\n\nWe can now create a new lifecycle configuration.\n\nDOMAIN_ID=$(echo \"$1\")\nUSER_PROFILE=$(echo \"$2\")\n\nLCC_CONTENT=`openssl base64 -A -in $3/autoshutdown.sh`\n\naws sagemaker delete-studio-lifecycle-config \\\n    --studio-lifecycle-config-name autoshutdown 2&gt; /dev/null\n\nresponse=$(aws sagemaker create-studio-lifecycle-config \\\n    --studio-lifecycle-config-name autoshutdown \\\n    --studio-lifecycle-config-content $LCC_CONTENT \\\n    --studio-lifecycle-config-app-type JupyterServer) \n\narn=$(echo \"${response}\" | python3 -c \"import sys, json; print(json.load(sys.stdin)['StudioLifecycleConfigArn'])\")\necho \"${arn}\"\n\naws sagemaker update-user-profile --domain-id $DOMAIN_ID \\\n    --user-profile-name $USER_PROFILE \\\n    --user-settings '{\n        \"JupyterServerAppSettings\": {\n            \"DefaultResourceSpec\": {\n                \"LifecycleConfigArn\": \"'$arn'\",\n                \"InstanceType\": \"system\"\n            },\n            \"LifecycleConfigArns\": [\"'$arn'\"]\n        }\n    }'"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Class Project",
    "section": "",
    "text": "The goal of this project is to build a training pipeline to preprocess, train, evaluate, and register a machine learning model.\nYou’ll start from the template pipeline that we discussed during the program and make the necessary changes to it. Before making any changes, ensure you can run the pipeline from Session 4 without issues.\nThe project has three different levels of complexity. Pick the one that you feel most comfortable tackling first."
  },
  {
    "objectID": "project.html#simple-complexity",
    "href": "project.html#simple-complexity",
    "title": "Class Project",
    "section": "Simple complexity",
    "text": "Simple complexity\nWe want to replace the Penguins dataset with a different classification problem. Feel free to use any dataset you like. If you don’t have any ideas, here are three options you can choose from:\n\nIris flowers dataset - This is a multi-class classification problem where you’ll predict the flower species given the measurements of iris flowers.\nAdult income dataset - This is a binary classification problem where you’ll predict whether the income of a person exceeds $50,000/yr based on census data.\nBanknote authentication dataset - This is a binary classification problem where you’ll predict whether a given banknote is authentic given the measures from a photograph.\n\nStart with the pipeline from Session 4 and modify the preprocessing, training, and evaluation scripts to use the new dataset."
  },
  {
    "objectID": "project.html#intermediate-complexity",
    "href": "project.html#intermediate-complexity",
    "title": "Class Project",
    "section": "Intermediate complexity",
    "text": "Intermediate complexity\nWe want to replace TensorFlow with PyTorch in the pipeline we built in Session 4. Everything else will stay the same, except the framework to train the model.\nStart with the pipeline from Session 4 and modify the training and evaluation scripts to train and evaluate the model using PyTorch. Notice you’ll need to use a PyTorch estimator to configure the Training Step and a PyTorch processor to configure the evaluation step."
  },
  {
    "objectID": "project.html#advanced-complexity",
    "href": "project.html#advanced-complexity",
    "title": "Class Project",
    "section": "Advanced complexity",
    "text": "Advanced complexity\nAt this stage, we want to combine replacing the Penguins dataset with replacing TensorFlow with PyTorch in the pipeline.\nStart with the pipeline from Session 4 and make the necessary changes described in the simple and intermediate complexity sections."
  },
  {
    "objectID": "cohort.html",
    "href": "cohort.html",
    "title": "Building Production Machine Learning Systems",
    "section": "",
    "text": "This notebook creates a SageMaker Pipeline to build an end-to-end Machine Learning system to solve the problem of classifying penguin species. With a SageMaker Pipeline, you can create, automate, and manage end-to-end Machine Learning workflows at scale.\nYou can find more information about Amazon SageMaker in the Amazon SageMaker Developer Guide. The AWS Machine Learning Blog is an excellent source to stay up-to-date with SageMaker.\nThis example uses the Penguins dataset, the boto3 library, and the SageMaker Python SDK.\nThis notebook is part of the Machine Learning School program."
  },
  {
    "objectID": "cohort.html#initial-setup",
    "href": "cohort.html#initial-setup",
    "title": "Building Production Machine Learning Systems",
    "section": "Initial setup",
    "text": "Initial setup\n\n\n\n\n\n\nNote\n\n\n\nBefore running this notebook, follow the Setup Instructions for the program.\n\n\nLet’s start by setting up the environment and preparing to run the notebook.\nWe can run this notebook in Local Mode to test the pipeline in your local environment before using SageMaker. You can run the code in Local Mode by setting the LOCAL_MODE constant to True.\n\nLOCAL_MODE = True\n\nLet’s load the S3 bucket name and the AWS Role from the environment variables:\n\nimport os\n\nbucket = os.environ[\"BUCKET\"]\nrole = os.environ[\"ROLE\"]\n\nS3_LOCATION = f\"s3://{bucket}/penguins\"\n\nIf you are running the pipeline in Local Mode on an ARM64 machine, you will need to use a custom Docker image to train and evaluate the model. This is because SageMaker doesn’t provide a TensorFlow image that supports Apple’s M chips.\n\narchitecture = !(uname -m)\nIS_APPLE_M_CHIP = architecture[0] == \"arm64\"\n\nLet’s create a configuration dictionary with different settings depending on whether we are running the pipeline in Local Mode or not:\n\nimport sagemaker\nfrom sagemaker.workflow.pipeline_context import PipelineSession, LocalPipelineSession\n\npipeline_session = PipelineSession(default_bucket=bucket) if not LOCAL_MODE else None\n\nif LOCAL_MODE:\n    config = {\n        \"session\": LocalPipelineSession(default_bucket=bucket),\n        \"instance_type\": \"local\",\n        # We need to use a custom Docker image when we run the pipeline\n        # in Local Model on an ARM64 machine.\n        \"image\": \"sagemaker-tensorflow-toolkit-local\" if IS_APPLE_M_CHIP else None,\n    }\nelse:\n    config = {\n        \"session\": pipeline_session,\n        \"instance_type\": \"ml.m5.xlarge\",\n        \"image\": None,\n    }\n\nconfig[\"framework_version\"] = \"2.11\"\nconfig[\"py_version\"] = \"py39\"\n\nLet’s now initialize a few variables that we’ll need throughout the notebook:\n\nimport boto3\n\nsagemaker_session = sagemaker.session.Session()\nsagemaker_client = boto3.client(\"sagemaker\")\niam_client = boto3.client(\"iam\")\nregion = boto3.Session().region_name"
  },
  {
    "objectID": "cohort.html#session-1---production-machine-learning-is-different",
    "href": "cohort.html#session-1---production-machine-learning-is-different",
    "title": "Building Production Machine Learning Systems",
    "section": "Session 1 - Production Machine Learning is Different",
    "text": "Session 1 - Production Machine Learning is Different\nIn this session we’ll run Exploratory Data Analysis on the Penguins dataset and we’ll build a simple SageMaker Pipeline with one step to split and transform the data.\n \nWe’ll use a Scikit-Learn Pipeline for the transformations, and a Processing Step with a SKLearnProcessor to execute a preprocessing script. Check the SageMaker Pipelines Overview for an introduction to the fundamental components of a SageMaker Pipeline.\n\nStep 1 - Exploratory Data Analysis\nLet’s run Exploratory Data Analysis on the dataset. The goal of this section is to understand the data and the problem we are trying to solve.\nLet’s load the Penguins dataset:\n\nimport pandas as pd\nimport numpy as np\n\npenguins = pd.read_csv(DATA_FILEPATH)\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMALE\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n\n\n\n\n\n\n\nWe can see the dataset contains the following columns:\n\nspecies: The species of a penguin. This is the column we want to predict.\nisland: The island where the penguin was found\nculmen_length_mm: The length of the penguin’s culmen (bill) in millimeters\nculmen_depth_mm: The depth of the penguin’s culmen in millimeters\nflipper_length_mm: The length of the penguin’s flipper in millimeters\nbody_mass_g: The body mass of the penguin in grams\nsex: The sex of the penguin\n\nIf you are curious, here is the description of a penguin’s culmen:\n\nNow, let’s get the summary statistics for the features in our dataset.\n\npenguins.describe(include=\"all\")\n\n\n\n\n\n\n\n\nspecies\nisland\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\ncount\n344\n344\n342.000000\n342.000000\n342.000000\n342.000000\n334\n\n\nunique\n3\n3\nNaN\nNaN\nNaN\nNaN\n3\n\n\ntop\nAdelie\nBiscoe\nNaN\nNaN\nNaN\nNaN\nMALE\n\n\nfreq\n152\n168\nNaN\nNaN\nNaN\nNaN\n168\n\n\nmean\nNaN\nNaN\n43.921930\n17.151170\n200.915205\n4201.754386\nNaN\n\n\nstd\nNaN\nNaN\n5.459584\n1.974793\n14.061714\n801.954536\nNaN\n\n\nmin\nNaN\nNaN\n32.100000\n13.100000\n172.000000\n2700.000000\nNaN\n\n\n25%\nNaN\nNaN\n39.225000\n15.600000\n190.000000\n3550.000000\nNaN\n\n\n50%\nNaN\nNaN\n44.450000\n17.300000\n197.000000\n4050.000000\nNaN\n\n\n75%\nNaN\nNaN\n48.500000\n18.700000\n213.000000\n4750.000000\nNaN\n\n\nmax\nNaN\nNaN\n59.600000\n21.500000\n231.000000\n6300.000000\nNaN\n\n\n\n\n\n\n\nLet’s now display the distribution of values for the three categorical columns in our data:\n\nspecies_distribution = penguins[\"species\"].value_counts()\nisland_distribution = penguins[\"island\"].value_counts()\nsex_distribution = penguins[\"sex\"].value_counts()\n\nprint(species_distribution)\nprint()\nprint(island_distribution)\nprint()\nprint(sex_distribution)\n\nspecies\nAdelie       152\nGentoo       124\nChinstrap     68\nName: count, dtype: int64\n\nisland\nBiscoe       168\nDream        124\nTorgersen     52\nName: count, dtype: int64\n\nsex\nMALE      168\nFEMALE    165\n.           1\nName: count, dtype: int64\n\n\nThe distribution of the categories in our data are:\n\nspecies: There are 3 species of penguins in the dataset: Adelie (152), Gentoo (124), and Chinstrap (68).\nisland: Penguins are from 3 islands: Biscoe (168), Dream (124), and Torgersen (52).\nsex: We have 168 male penguins, 165 female penguins, and 1 penguin with an ambiguous gender (.).\n\nLet’s replace the ambiguous value in the sex column with a null value:\n\npenguins[\"sex\"] = penguins[\"sex\"].replace(\".\", np.nan)\nsex_distribution = penguins[\"sex\"].value_counts()\nsex_distribution\n\nsex\nMALE      168\nFEMALE    165\nName: count, dtype: int64\n\n\nNext, let’s check for any missing values in the dataset.\n\npenguins.isnull().sum()\n\nspecies               0\nisland                0\nculmen_length_mm      2\nculmen_depth_mm       2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\nLet’s get rid of the missing values. For now, we are going to replace the missing values with the most frequent value in the column. Later, we’ll use a different strategy to replace missing numeric values.\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy=\"most_frequent\")\npenguins.iloc[:, :] = imputer.fit_transform(penguins)\npenguins.isnull().sum()\n\nspecies              0\nisland               0\nculmen_length_mm     0\nculmen_depth_mm      0\nflipper_length_mm    0\nbody_mass_g          0\nsex                  0\ndtype: int64\n\n\nLet’s visualize the distribution of categorical features.\n\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(3, 1, figsize=(6, 10))\n\naxs[0].bar(species_distribution.index, species_distribution.values)\naxs[0].set_ylabel(\"Count\")\naxs[0].set_title(\"Distribution of Species\")\n\naxs[1].bar(island_distribution.index, island_distribution.values)\naxs[1].set_ylabel(\"Count\")\naxs[1].set_title(\"Distribution of Island\")\n\naxs[2].bar(sex_distribution.index, sex_distribution.values)\naxs[2].set_ylabel(\"Count\")\naxs[2].set_title(\"Distribution of Sex\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nLet’s visualize the distribution of numerical columns.\n\nfig, axs = plt.subplots(2, 2, figsize=(8, 6))\n\naxs[0, 0].hist(penguins[\"culmen_length_mm\"], bins=20)\naxs[0, 0].set_ylabel(\"Count\")\naxs[0, 0].set_title(\"Distribution of culmen_length_mm\")\n\naxs[0, 1].hist(penguins[\"culmen_depth_mm\"], bins=20)\naxs[0, 1].set_ylabel(\"Count\")\naxs[0, 1].set_title(\"Distribution of culmen_depth_mm\")\n\naxs[1, 0].hist(penguins[\"flipper_length_mm\"], bins=20)\naxs[1, 0].set_ylabel(\"Count\")\naxs[1, 0].set_title(\"Distribution of flipper_length_mm\")\n\naxs[1, 1].hist(penguins[\"body_mass_g\"], bins=20)\naxs[1, 1].set_ylabel(\"Count\")\naxs[1, 1].set_title(\"Distribution of body_mass_g\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nLet’s display the covariance matrix of the dataset. The “covariance” measures how changes in one variable are associated with changes in a second variable. In other words, the covariance measures the degree to which two variables are linearly associated.\n\npenguins.cov(numeric_only=True)\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nculmen_length_mm\n29.679415\n-2.516984\n50.260588\n2596.971151\n\n\nculmen_depth_mm\n-2.516984\n3.877201\n-16.108849\n-742.660180\n\n\nflipper_length_mm\n50.260588\n-16.108849\n197.269501\n9792.552037\n\n\nbody_mass_g\n2596.971151\n-742.660180\n9792.552037\n640316.716388\n\n\n\n\n\n\n\nHere are three examples of what we get from interpreting the covariance matrix below:\n\nPenguins that weight more tend to have a larger culmen.\nThe more a penguin weights, the shallower its culmen tends to be.\nThere’s a small variance between the culmen depth of penguins.\n\nLet’s now display the correlation matrix. “Correlation” measures both the strength and direction of the linear relationship between two variables.\n\npenguins.corr(numeric_only=True)\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nculmen_length_mm\n1.000000\n-0.234635\n0.656856\n0.595720\n\n\nculmen_depth_mm\n-0.234635\n1.000000\n-0.582472\n-0.471339\n\n\nflipper_length_mm\n0.656856\n-0.582472\n1.000000\n0.871302\n\n\nbody_mass_g\n0.595720\n-0.471339\n0.871302\n1.000000\n\n\n\n\n\n\n\nHere are three examples of what we get from interpreting the correlation matrix below:\n\nPenguins that weight more tend to have larger flippers.\nPenguins with a shallower culmen tend to have larger flippers.\nThe length and depth of the culmen have a slight negative correlation.\n\nLet’s display the distribution of species by island.\n\nunique_species = penguins[\"species\"].unique()\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfor species in unique_species:\n    data = penguins[penguins[\"species\"] == species]\n    ax.hist(data[\"island\"], bins=5, alpha=0.5, label=species)\n\nax.set_xlabel(\"Island\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Distribution of Species by Island\")\nax.legend()\nplt.show()\n\n\n\n\nLet’s display the distribution of species by sex.\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nfor species in unique_species:\n    data = penguins[penguins[\"species\"] == species]\n    ax.hist(data[\"sex\"], bins=3, alpha=0.5, label=species)\n\nax.set_xlabel(\"Sex\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Distribution of Species by Sex\")\n\nax.legend()\nplt.show()\n\n\n\n\n\n\nStep 2 - Creating the Preprocessing Script\nHere’s a high-level overview of the preprocessing step and the Processing Job that SageMaker creates behind the scenes:\n \nThe first step we need in the pipeline is a Processing Step to run a script that will split and transform the data. This Processing Step will create a SageMaker Processing Job in the background, run the script, and upload the output to S3. You can use Processing Jobs to perform data preprocessing, post-processing, feature engineering, data validation, and model evaluation. Check the ProcessingStep SageMaker’s SDK documentation for more information.\nThe first step is to create the script that will split and transform the input data.\n\n\n\npreprocessor.py\n\nimport os\nimport tarfile\nimport tempfile\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n\n\ndef preprocess(base_directory):\n    \"\"\"\n    This function loads the supplied data, splits it and transforms it.\n    \"\"\"\n\n    df = _read_data_from_input_csv_files(base_directory)\n    \n    target_transformer = ColumnTransformer(\n        transformers=[(\"species\", OrdinalEncoder(), [0])]\n    )\n    \n    numeric_transformer = make_pipeline(\n        SimpleImputer(strategy=\"mean\"),\n        StandardScaler()\n    )\n\n    categorical_transformer = make_pipeline(\n        SimpleImputer(strategy=\"most_frequent\"),\n        OneHotEncoder()\n    )\n    \n    features_transformer = ColumnTransformer(\n        transformers=[\n            (\"numeric\", numeric_transformer, make_column_selector(dtype_exclude=\"object\")),\n            (\"categorical\", categorical_transformer, [\"island\"]),\n        ]\n    )\n\n    df_train, df_validation, df_test = _split_data(df)\n\n    _save_baselines(base_directory, df_train, df_test)\n\n    y_train = target_transformer.fit_transform(np.array(df_train.species.values).reshape(-1, 1))\n    y_validation = target_transformer.transform(np.array(df_validation.species.values).reshape(-1, 1))\n    y_test = target_transformer.transform(np.array(df_test.species.values).reshape(-1, 1))\n    \n    df_train = df_train.drop(\"species\", axis=1)\n    df_validation = df_validation.drop(\"species\", axis=1)\n    df_test = df_test.drop(\"species\", axis=1)\n\n    X_train = features_transformer.fit_transform(df_train)\n    X_validation = features_transformer.transform(df_validation)\n    X_test = features_transformer.transform(df_test)\n\n    _save_splits(base_directory, X_train, y_train, X_validation, y_validation, X_test, y_test)\n    _save_model(base_directory, target_transformer, features_transformer)\n    \n\ndef _read_data_from_input_csv_files(base_directory):\n    \"\"\"\n    This function reads every CSV file available and concatenates\n    them into a single dataframe.\n    \"\"\"\n\n    input_directory = Path(base_directory) / \"input\"\n    files = [file for file in input_directory.glob(\"*.csv\")]\n    \n    if len(files) == 0:\n        raise ValueError(f\"The are no CSV files in {str(input_directory)}/\")\n        \n    raw_data = [pd.read_csv(file) for file in files]\n    df = pd.concat(raw_data)\n    \n    # Shuffle the data\n    return df.sample(frac=1, random_state=42)\n\n\ndef _split_data(df):\n    \"\"\"\n    Splits the data into three sets: train, validation and test.\n    \"\"\"\n\n    df_train, temp = train_test_split(df, test_size=0.3)\n    df_validation, df_test = train_test_split(temp, test_size=0.5)\n\n    return df_train, df_validation, df_test\n\n\ndef _save_baselines(base_directory, df_train, df_test):\n    \"\"\"\n    During the data and quality monitoring steps, we will need baselines\n    to compute constraints and statistics. This function saves the \n    untransformed data to disk so we can use them as baselines later.\n    \"\"\"\n\n    for split, data in [(\"train\", df_train), (\"test\", df_test)]:\n        baseline_path = Path(base_directory) / f\"{split}-baseline\"\n        baseline_path.mkdir(parents=True, exist_ok=True)\n\n        df = data.copy().dropna()\n\n        # We want to save the header only for the train baseline\n        # but not for the test baseline. We'll use the test baseline\n        # to generate predictions later, and we can't have a header line\n        # because the model won't be able to make a prediction for it.\n        header = split == \"train\"\n        df.to_csv(baseline_path / f\"{split}-baseline.csv\", header=header, index=False)\n\n\ndef _save_splits(base_directory, X_train, y_train, X_validation, y_validation, X_test, y_test):\n    \"\"\"\n    This function concatenates the transformed features and the target variable, and\n    saves each one of the split sets to disk.\n    \"\"\"\n\n    train = np.concatenate((X_train, y_train), axis=1)\n    validation = np.concatenate((X_validation, y_validation), axis=1)\n    test = np.concatenate((X_test, y_test), axis=1)\n\n    train_path = Path(base_directory) / \"train\"\n    validation_path = Path(base_directory) / \"validation\"\n    test_path = Path(base_directory) / \"test\"\n\n    train_path.mkdir(parents=True, exist_ok=True)\n    validation_path.mkdir(parents=True, exist_ok=True)\n    test_path.mkdir(parents=True, exist_ok=True)\n\n    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n    pd.DataFrame(validation).to_csv(validation_path / \"validation.csv\", header=False, index=False)\n    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n\n\ndef _save_model(base_directory, target_transformer, features_transformer):\n    \"\"\"\n    This function creates a model.tar.gz file that contains the two transformation\n    pipelines we built to transform the data.\n    \"\"\"\n\n    with tempfile.TemporaryDirectory() as directory:\n        joblib.dump(target_transformer, os.path.join(directory, \"target.joblib\"))\n        joblib.dump(features_transformer, os.path.join(directory, \"features.joblib\"))\n    \n        model_path = Path(base_directory) / \"model\"\n        model_path.mkdir(parents=True, exist_ok=True)\n    \n        with tarfile.open(f\"{str(model_path / 'model.tar.gz')}\", \"w:gz\") as tar:\n            tar.add(os.path.join(directory, \"target.joblib\"), arcname=\"target.joblib\")\n            tar.add(os.path.join(directory, \"features.joblib\"), arcname=\"features.joblib\")\n\n    \nif __name__ == \"__main__\":\n    preprocess(base_directory=\"/opt/ml/processing\")\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nimport os\nimport shutil\nimport tarfile\nimport pytest\nimport tempfile\nimport joblib\nfrom preprocessor import preprocess\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    preprocess(base_directory=directory)\n    \n    yield directory\n    \n    shutil.rmtree(directory)\n\n\ndef test_preprocess_generates_data_splits(directory):\n    output_directories = os.listdir(directory)\n    \n    assert \"train\" in output_directories\n    assert \"validation\" in output_directories\n    assert \"test\" in output_directories\n\n\ndef test_preprocess_generates_baselines(directory):\n    output_directories = os.listdir(directory)\n\n    assert \"train-baseline\" in output_directories\n    assert \"test-baseline\" in output_directories\n\n\ndef test_preprocess_creates_two_models(directory):\n    model_path = directory / \"model\"\n    tar = tarfile.open(model_path / \"model.tar.gz\", \"r:gz\")\n\n    assert \"features.joblib\" in tar.getnames()\n    assert \"target.joblib\" in tar.getnames()\n\n\ndef test_splits_are_transformed(directory):\n    train = pd.read_csv(directory / \"train\" / \"train.csv\", header=None)\n    validation = pd.read_csv(directory / \"validation\" / \"validation.csv\", header=None)\n    test = pd.read_csv(directory / \"test\" / \"test.csv\", header=None)\n\n    # After transforming the data, the number of features should be 7:\n    # * 3 - island (one-hot encoded)\n    # * 1 - culmen_length_mm = 1\n    # * 1 - culmen_depth_mm\n    # * 1 - flipper_length_mm\n    # * 1 - body_mass_g\n    number_of_features = 7\n\n    # The transformed splits should have an additional column for the target\n    # variable.\n    assert train.shape[1] == number_of_features + 1\n    assert validation.shape[1] == number_of_features + 1\n    assert test.shape[1] == number_of_features + 1\n\n\ndef test_train_baseline_is_not_transformed(directory):\n    baseline = pd.read_csv(directory / \"train-baseline\" / \"train-baseline.csv\", header=None)\n\n    island = baseline.iloc[:, 1].unique()\n\n    assert \"Biscoe\" in island\n    assert \"Torgersen\" in island\n    assert \"Dream\" in island\n\n\ndef test_test_baseline_is_not_transformed(directory):\n    baseline = pd.read_csv(directory / \"test-baseline\" / \"test-baseline.csv\", header=None)\n\n    island = baseline.iloc[:, 1].unique()\n\n    assert \"Biscoe\" in island\n    assert \"Torgersen\" in island\n    assert \"Dream\" in island\n\n\ndef test_train_baseline_includes_header(directory):\n    baseline = pd.read_csv(directory / \"train-baseline\" / \"train-baseline.csv\")\n    assert baseline.columns[0] == \"species\"\n\n\ndef test_test_baseline_does_not_include_header(directory):\n    baseline = pd.read_csv(directory / \"test-baseline\" / \"test-baseline.csv\")\n    assert baseline.columns[0] != \"species\"\n\n\n\n\nStep 3 - Setting up the Processing Step\nLet’s now define the ProcessingStep that we’ll use in the pipeline to run the script that will split and transform the data.\nSeveral SageMaker Pipeline steps support caching. When a step runs, and dependending on the configured caching policy, SageMaker will try to reuse the result of a previous successful run of the same step. You can find more information about this topic in Caching Pipeline Steps. Let’s define a caching policy that we’ll reuse on every step:\n\nfrom sagemaker.workflow.steps import CacheConfig\n\ncache_config = CacheConfig(enable_caching=True, expire_after=\"15d\")\n\nWe can parameterize a SageMaker Pipeline to make it more flexible. In this case, we’ll use a parameter to pass the location of the dataset we want to process. We can execute the pipeline with different datasets by changing the value of this parameter. To read more about these parameters, check Pipeline Parameters.\n\nfrom sagemaker.workflow.parameters import ParameterString\n\ndataset_location = ParameterString(\n    name=\"dataset_location\",\n    default_value=f\"{S3_LOCATION}/data\",\n)\n\nA processor gives the Processing Step information about the hardware and software that SageMaker should use to launch the Processing Job. To run the script we created, we need access to Scikit-Learn, so we can use the SKLearnProcessor processor that comes out-of-the-box with the SageMaker’s Python SDK.\nSageMaker manages the infrastructure of a Processing Job. It provisions resources for the duration of the job, and cleans up when it completes. The Processing Container image that SageMaker uses to run a Processing Job can either be a SageMaker built-in image or a custom image.\nThe Data Processing with Framework Processors page discusses other built-in processors you can use. The Docker Registry Paths and Example Code page contains information about the available framework versions for each region.\n\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nprocessor = SKLearnProcessor(\n    base_job_name=\"preprocess-data\",\n    framework_version=\"1.2-1\",\n    # By default, a new account doesn't have access to `ml.m5.xlarge` instances.\n    # If you haven't requested a quota increase yet, you can use an\n    # `ml.t3.medium` instance type instead. This will work out of the box, but\n    # the Processing Job will take significantly longer than it should have.\n    # To get access to `ml.m5.xlarge` instances, you can request a quota\n    # increase under the Service Quotas section in your AWS account.\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    role=role,\n    sagemaker_session=config[\"session\"],\n)\n\nLet’s now define the Processing Step that we’ll use in the pipeline. This step requires a list of inputs that we need on the preprocessing script. In this case, the input is the dataset we stored in S3. We also have a few outputs that we want SageMaker to capture when the Processing Job finishes.\n\nfrom sagemaker.workflow.steps import ProcessingStep\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\n\npreprocessing_step = ProcessingStep(\n    name=\"preprocess-data\",\n    step_args=processor.run(\n        code=f\"{CODE_FOLDER}/preprocessor.py\",\n        inputs=[\n            ProcessingInput(\n                source=dataset_location, destination=\"/opt/ml/processing/input\"\n            ),\n        ],\n        outputs=[\n            ProcessingOutput(\n                output_name=\"train\",\n                source=\"/opt/ml/processing/train\",\n                destination=f\"{S3_LOCATION}/preprocessing/train\",\n            ),\n            ProcessingOutput(\n                output_name=\"validation\",\n                source=\"/opt/ml/processing/validation\",\n                destination=f\"{S3_LOCATION}/preprocessing/validation\",\n            ),\n            ProcessingOutput(\n                output_name=\"test\",\n                source=\"/opt/ml/processing/test\",\n                destination=f\"{S3_LOCATION}/preprocessing/test\",\n            ),\n            ProcessingOutput(\n                output_name=\"model\",\n                source=\"/opt/ml/processing/model\",\n                destination=f\"{S3_LOCATION}/preprocessing/model\",\n            ),\n            ProcessingOutput(\n                output_name=\"train-baseline\",\n                source=\"/opt/ml/processing/train-baseline\",\n                destination=f\"{S3_LOCATION}/preprocessing/train-baseline\",\n            ),\n            ProcessingOutput(\n                output_name=\"test-baseline\",\n                source=\"/opt/ml/processing/test-baseline\",\n                destination=f\"{S3_LOCATION}/preprocessing/test-baseline\",\n            ),\n        ],\n    ),\n    cache_config=cache_config,\n)\n\n\n\nStep 4 - Creating the Pipeline\nWe can now create the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nfrom sagemaker.workflow.pipeline import Pipeline\nfrom sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n\npipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n\nsession1_pipeline = Pipeline(\n    name=\"session1-pipeline\",\n    parameters=[dataset_location],\n    steps=[\n        preprocessing_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession1_pipeline.upsert(role_arn=role)\n\nWe can now start the pipeline:\n\nsession1_pipeline.start()\n\n\n\nAssignments\n\nAssignment 1.1 For this assignment, you should run the pipeline on your environment using Local Mode and then switch it to run in SageMaker. After completing this assignment, you should have your environment fully configured and your pipeline running without any issues. This assignment is fundamental to the rest of the program, so make sure you complete it before moving on to any other assignments.\nAssignment 1.2 The pipeline uses Random Sampling to split the dataset. Modify the code to use Stratified Sampling instead. The goal of this assignment is to help you familiarize with how to modify the preprocessing script and re-run the pipeline to see your changes in action.\nAssignment 1.3 We can specify different parameter values in a pipeline at the time we start it. In this session, we defined a dataset_location parameter that specifies the location of the data that we want the pipeline to process. For this assignment, use ChatGPT to generate dataset with 500 random penguins and store the file in S3. Then, run the pipeline pointing the dataset_location to the new dataset. Here is an explanation of how to override default parameters during a pipeline execution. You can use the Advanced Data Analysis tool from ChatGPT to generate the fake data. If you don’t have access to it, you can simply duplicate your dataset and store it at a different S3 location.\nAssignment 1.4 For this assignment, we want to run a distributed Processing Job across multiple instances to capitalize the island column of the dataset. Your dataset will consist of 10 different files stored in S3. Set up a Processing Step using two instances. When specifying the input to the Processing Step, you must set the ProcessingInput.s3_data_distribution_type attribute to ShardedByS3Key. By doing this, SageMaker will run a cluster with two instances simultaneously, each with access to half the files. Check the S3DataDistributionType documentation for more information.\nAssignment 1.5 You can use Amazon SageMaker Data Wrangler to complete each step of the data preparation workflow (including data selection, cleansing, exploration, visualization, and processing at scale) from a single visual interface. For this assignment, load the Data Wrangler interface and use it to build the same transformations we implemented using the Scikit-Learn Pipeline. If you have questions, open the Penguins Data Flow included in this repository."
  },
  {
    "objectID": "cohort.html#session-2---building-models-and-the-training-pipeline",
    "href": "cohort.html#session-2---building-models-and-the-training-pipeline",
    "title": "Building Production Machine Learning Systems",
    "section": "Session 2 - Building Models And The Training Pipeline",
    "text": "Session 2 - Building Models And The Training Pipeline\nThis session extends the SageMaker Pipeline we built in the previous session with a step to train a model. We’ll explore the Training Step and the Tuning Step.\n \nWe’ll introduce Amazon SageMaker Experiments and use them during training. For more information about this topic, check the SageMaker Experiments’ SDK documentation.\n\nStep 1 - Creating the Training Script\nHere’s a high-level overview of the training step and the Training Job that SageMaker creates behind the scenes:\n \nThis following script is responsible for training a neural network using the train data, validating the model, and saving it so we can later use it:\n\n\n\ntrain.py\n\nimport os\nimport argparse\n\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom sklearn.metrics import accuracy_score\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\n\n\ndef train(model_directory, train_path, validation_path, epochs=50, batch_size=32):\n    X_train = pd.read_csv(Path(train_path) / \"train.csv\")\n    y_train = X_train[X_train.columns[-1]]\n    X_train.drop(X_train.columns[-1], axis=1, inplace=True)\n    \n    X_validation = pd.read_csv(Path(validation_path) / \"validation.csv\")\n    y_validation = X_validation[X_validation.columns[-1]]\n    X_validation.drop(X_validation.columns[-1], axis=1, inplace=True)\n        \n    model = Sequential([\n        Dense(10, input_shape=(X_train.shape[1],), activation=\"relu\"),\n        Dense(8, activation=\"relu\"),\n        Dense(3, activation=\"softmax\"),\n    ])\n    \n    model.compile(\n        optimizer=SGD(learning_rate=0.01),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n\n    model.fit(\n        X_train, \n        y_train, \n        validation_data=(X_validation, y_validation),\n        epochs=epochs, \n        batch_size=batch_size,\n        verbose=2,\n    )\n\n    predictions = np.argmax(model.predict(X_validation), axis=-1)\n    print(f\"Validation accuracy: {accuracy_score(y_validation, predictions)}\")\n    \n    model_filepath = Path(model_directory) / \"001\"\n    model.save(model_filepath)    \n    \n\nif __name__ == \"__main__\":\n    # Any hyperparameters provided by the training job are passed to \n    # the entry point as script arguments. \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--epochs\", type=int, default=50)\n    parser.add_argument(\"--batch_size\", type=int, default=32)\n    args, _ = parser.parse_known_args()\n\n    train(\n        # This is the location where we need to save our model. SageMaker will\n        # create a model.tar.gz file with anything inside this directory when\n        # the training script finishes.\n        model_directory=os.environ[\"SM_MODEL_DIR\"],\n\n        # SageMaker creates one channel for each one of the inputs to the\n        # Training Step.\n        train_path=os.environ[\"SM_CHANNEL_TRAIN\"],\n        validation_path=os.environ[\"SM_CHANNEL_VALIDATION\"],\n\n        epochs=args.epochs,\n        batch_size=args.batch_size,\n    )\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nimport os\nimport shutil\nimport tarfile\nimport pytest\nimport tempfile\nimport joblib\n\nfrom preprocessor import preprocess\nfrom train import train\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    \n    preprocess(base_directory=directory)\n    train(\n        model_directory=directory / \"model\",\n        train_path=directory / \"train\", \n        validation_path=directory / \"validation\",\n        epochs=1\n    )\n    \n    yield directory\n    \n    shutil.rmtree(directory)\n\n\ndef test_train_saves_a_folder_with_model_assets(directory):\n    output = os.listdir(directory / \"model\")\n    assert \"001\" in output\n    \n    assets = os.listdir(directory / \"model\" / \"001\")\n    assert \"saved_model.pb\" in assets\n\n\n\n\nStep 2 - Setting up the Training Step\nWe can now create a Training Step that we can add to the pipeline. This Training Step will create a SageMaker Training Job in the background, run the training script, and upload the output to S3. Check the TrainingStep SageMaker’s SDK documentation for more information.\nSageMaker uses the concept of an Estimator to handle end-to-end training and deployment tasks. For this example, we will use the built-in TensorFlow Estimator to run the training script we wrote before.\nSageMaker manages the infrastructure of a Training Job. It provisions resources for the duration of the job, and cleans up when it completes. The Training Container image that SageMaker uses to run a Training Job can either be a SageMaker built-in image or a custom image.\nThe Docker Registry Paths and Example Code page contains information about the available framework versions for each region. Here, you can also check the available SageMaker Deep Learning Container images.\nNotice the list of hyperparameters defined below. SageMaker will pass these hyperparameters as arguments to the entry point of the training script.\nWe are going to use SageMaker Experiments to log information from the Training Job. For more information, check Manage Machine Learning with Amazon SageMaker Experiments. The list of metric definitions will tell SageMaker which metrics to track and how to parse them from the Training Job logs.\n\nfrom sagemaker.tensorflow import TensorFlow\n\nestimator = TensorFlow(\n    base_job_name=\"training\",\n    entry_point=f\"{CODE_FOLDER}/train.py\",\n    # SageMaker will pass these hyperparameters as arguments\n    # to the entry point of the training script.\n    hyperparameters={\n        \"epochs\": 50,\n        \"batch_size\": 32,\n    },\n    # SageMaker will track these metrics as part of the experiment\n    # associated to this pipeline. The metric definitions tells\n    # SageMaker how to parse the values from the Training Job logs.\n    metric_definitions=[\n        {\"Name\": \"loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\"},\n        {\"Name\": \"accuracy\", \"Regex\": \"accuracy: ([0-9\\\\.]+)\"},\n        {\"Name\": \"val_loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n        {\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"},\n    ],\n    image_uri=config[\"image\"],\n    framework_version=config[\"framework_version\"],\n    py_version=config[\"py_version\"],\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    disable_profiler=True,\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\nWe can now create a Training Step. This Training Step will create a SageMaker Training Job in the background, run the training script, and upload the output to S3. Check the TrainingStep SageMaker’s SDK documentation for more information.\nThis step will receive the train and validation split from the previous step as inputs.\nHere, we are using two input channels, train and validation. SageMaker will automatically create an environment variable corresponding to each of these channels following the format SM_CHANNEL_[channel_name]:\n\nSM_CHANNEL_TRAIN: This environment variable will contain the path to the data in the train channel\nSM_CHANNEL_VALIDATION: This environment variable will contain the path to the data in the validation channel\n\n\nfrom sagemaker.workflow.steps import TrainingStep\nfrom sagemaker.inputs import TrainingInput\n\ntrain_model_step = TrainingStep(\n    name=\"train-model\",\n    step_args=estimator.fit(\n        inputs={\n            \"train\": TrainingInput(\n                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n                    \"train\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\",\n            ),\n            \"validation\": TrainingInput(\n                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n                    \"validation\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\",\n            ),\n        }\n    ),\n    cache_config=cache_config,\n)\n\n\n\nStep 3 - Setting up a Tuning Step\nLet’s now create a Tuning Step. This Tuning Step will create a SageMaker Hyperparameter Tuning Job in the background and use the training script to train different model variants and choose the best one. Check the TuningStep SageMaker’s SDK documentation for more information.\nSince we could use the Training of the Tuning Step to create the model, we’ll define this constant to indicate which approach we want to run. Notice that the Tuning Step is not supported in Local Mode.\n\nUSE_TUNING_STEP = False and not LOCAL_MODE\n\nThe Tuning Step requires a HyperparameterTuner reference to configure the Hyperparameter Tuning Job.\nHere is the configuration that we’ll use to find the best model:\n\nobjective_metric_name: This is the name of the metric the tuner will use to determine the best model.\nobjective_type: This is the objective of the tuner. It specifies whether it should minimize the metric or maximize it. In this example, since we are using the validation accuracy of the model, we want the objective to be “Maximize.” If we were using the loss of the model, we would set the objective to “Minimize.”\nmetric_definitions: Defines how the tuner will determine the metric’s value by looking at the output logs of the training process.\n\nThe tuner expects the list of the hyperparameters you want to explore. You can use subclasses of the Parameter class to specify different types of hyperparameters. This example explores different values for the epochs hyperparameter.\nFinally, you can control the number of jobs and how many of them will run in parallel using the following two arguments:\n\nmax_jobs: Defines the maximum total number of training jobs to start for the hyperparameter tuning job.\nmax_parallel_jobs: Defines the maximum number of parallel training jobs to start.\n\n\nfrom sagemaker.tuner import HyperparameterTuner\nfrom sagemaker.parameter import IntegerParameter\n\ntuner = HyperparameterTuner(\n    estimator,\n    objective_metric_name=\"val_accuracy\",\n    objective_type=\"Maximize\",\n    hyperparameter_ranges={\n        \"epochs\": IntegerParameter(10, 50),\n    },\n    metric_definitions=[{\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}],\n    max_jobs=3,\n    max_parallel_jobs=3,\n)\n\nWe can now create the Tuning Step using the tuner we configured before.\nHere’s a high-level overview of this step and the Hyperparameter Tuning Job that SageMaker creates behind the scenes:\n \n\nfrom sagemaker.workflow.steps import TuningStep\n\ntune_model_step = TuningStep(\n    name=\"tune-model\",\n    step_args=tuner.fit(\n        inputs={\n            \"train\": TrainingInput(\n                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n                    \"train\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\",\n            ),\n            \"validation\": TrainingInput(\n                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n                    \"validation\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\",\n            ),\n        },\n    ),\n    cache_config=cache_config,\n)\n\n\n\nStep 4 - Creating the Pipeline\nLet’s define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession2_pipeline = Pipeline(\n    name=\"session2-pipeline\",\n    parameters=[dataset_location],\n    steps=[\n        preprocessing_step,\n        tune_model_step if USE_TUNING_STEP else train_model_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession2_pipeline.upsert(role_arn=role)\n\nWe can now start the pipeline:\n\nsession2_pipeline.start()\n\n\n\nAssignments\n\nAssignment 2.1 The training script trains the model using a hard-coded learning rate value. Modify the script to accept the learning rate as a parameter we can pass from the pipeline.\nAssignment 2.2 We currently define the number of epochs to train the model as a constant that we pass to the Estimator using the list of hyperparameters. Replace this constant with a new Pipeline Parameter named training_epochs.\nAssignment 2.3 The current tuning process aims to find the model with the highest validation accuracy. Modify the code so the best model is the one with the lowest training loss.\nAssignment 2.4 We used an instance of SKLearnProcessor to run the script that transforms and splits the data. While this processor is convenient, it doesn’t allow us to install additional libraries in the container. Modify the code to use an instance of FrameworkProcessor instead SKLearnProcessor. This class will allow us to specify a directory containing a requirements.txt file listing any additional dependencies. SageMaker will install these libraries in the processing container before triggering the processing job.\nAssignment 2.5 We configured the Training Step to log information from the Training Job as part of the SageMaker Experiment associated to the pipeline. As part of this assignment, check Manage Machine Learning with Amazon SageMaker Experiments and explore the generated experiments in the SageMaker Studio Console so you can familiarize with the information SageMaker logs during training."
  },
  {
    "objectID": "cohort.html#session-3---evaluating-and-versioning-models",
    "href": "cohort.html#session-3---evaluating-and-versioning-models",
    "title": "Building Production Machine Learning Systems",
    "section": "Session 3 - Evaluating and Versioning Models",
    "text": "Session 3 - Evaluating and Versioning Models\nThis session extends the SageMaker Pipeline with a step to evaluate the model and register it if it reaches a predefined accuracy threshold.\n \nWe’ll use a Processing Step to execute an evaluation script. We’ll use a Condition Step to determine whether the model’s accuracy is above a threshold, and a Model Step to register the model in the SageMaker Model Registry.\n\nStep 1 - Creating the Evaluation Script\nHere’s a high-level overview of the evaluation step and the Processing Job that SageMaker creates behind the scenes:\n \nLet’s create the evaluation script. The Processing Step will spin up a Processing Job and run this script inside a container. This script is responsible for loading the model we created and evaluating it on the test set. Before finishing, this script will generate an evaluation report of the model.\n\n\n\nevaluation.py\n\nimport json\nimport tarfile\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom tensorflow import keras\nfrom sklearn.metrics import accuracy_score\n\n\ndef evaluate(model_path, test_path, output_path):\n    X_test = pd.read_csv(Path(test_path) / \"test.csv\")\n    y_test = X_test[X_test.columns[-1]]\n    X_test.drop(X_test.columns[-1], axis=1, inplace=True)\n\n    # Let's now extract the model package so we can load \n    # it in memory.\n    with tarfile.open(Path(model_path) / \"model.tar.gz\") as tar:\n        tar.extractall(path=Path(model_path))\n        \n    model = keras.models.load_model(Path(model_path) / \"001\")\n    \n    predictions = np.argmax(model.predict(X_test), axis=-1)\n    accuracy = accuracy_score(y_test, predictions)\n    print(f\"Test accuracy: {accuracy}\")\n\n    # Let's create an evaluation report using the model accuracy.\n    evaluation_report = {\n        \"metrics\": {\n            \"accuracy\": {\n                \"value\": accuracy\n            },\n        },\n    }\n    \n    Path(output_path).mkdir(parents=True, exist_ok=True)\n    with open(Path(output_path) / \"evaluation.json\", \"w\") as f:\n        f.write(json.dumps(evaluation_report))\n        \n        \nif __name__ == \"__main__\":\n    evaluate(\n        model_path=\"/opt/ml/processing/model/\", \n        test_path=\"/opt/ml/processing/test/\",\n        output_path=\"/opt/ml/processing/evaluation/\"\n    )\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nimport os\nimport shutil\nimport tarfile\nimport pytest\nimport tempfile\nimport joblib\n\nfrom preprocessor import preprocess\nfrom train import train\nfrom evaluation import evaluate\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    \n    preprocess(base_directory=directory)\n    \n    train(\n        model_directory=directory / \"model\",\n        train_path=directory / \"train\", \n        validation_path=directory / \"validation\",\n        epochs=1\n    )\n    \n    # After training a model, we need to prepare a package just like\n    # SageMaker would. This package is what the evaluation script is\n    # expecting as an input.\n    with tarfile.open(directory / \"model.tar.gz\", \"w:gz\") as tar:\n        tar.add(directory / \"model\" / \"001\", arcname=\"001\")\n        \n    evaluate(\n        model_path=directory, \n        test_path=directory / \"test\",\n        output_path=directory / \"evaluation\",\n    )\n\n    yield directory / \"evaluation\"\n    \n    shutil.rmtree(directory)\n\n\ndef test_evaluate_generates_evaluation_report(directory):\n    output = os.listdir(directory)\n    assert \"evaluation.json\" in output\n\n\ndef test_evaluation_report_contains_accuracy(directory):\n    with open(directory / \"evaluation.json\", 'r') as file:\n        report = json.load(file)\n        \n    assert \"metrics\" in report\n    assert \"accuracy\" in report[\"metrics\"]\n\n\n\n\nStep 2 - Setting up the Evaluation Step\nTo run the evaluation script, we will use a Processing Step configured with TensorFlowProcessor because the script needs access to TensorFlow.\n\nfrom sagemaker.tensorflow import TensorFlowProcessor\n\nevaluation_processor = TensorFlowProcessor(\n    base_job_name=\"evaluation-processor\",\n    image_uri=config[\"image\"],\n    framework_version=config[\"framework_version\"],\n    py_version=config[\"py_version\"],\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    role=role,\n    sagemaker_session=config[\"session\"],\n)\n\nOne of the inputs to the Evaluation Step will be the model assets. We can use the USE_TUNING_STEP flag to determine whether we created the model using a Training Step or a Tuning Step. In case we are using the Tuning Step, we can use the TuningStep.get_top_model_s3_uri() function to get the model assets from the top performing training job of the Hyperparameter Tuning Job.\n\nmodel_assets = train_model_step.properties.ModelArtifacts.S3ModelArtifacts\n\nif USE_TUNING_STEP:\n    model_assets = tune_model_step.get_top_model_s3_uri(\n        top_k=0, s3_bucket=config[\"session\"].default_bucket()\n    )\n\nSageMaker supports mapping outputs to property files. This is useful when accessing a specific property from the pipeline. In our case, we want to access the accuracy of the model in the Condition Step, so we’ll map the evaluation report to a property file. Check How to Build and Manage Property Files for more information.\n\nfrom sagemaker.workflow.properties import PropertyFile\n\nevaluation_report = PropertyFile(\n    name=\"evaluation-report\", output_name=\"evaluation\", path=\"evaluation.json\"\n)\n\nWe are now ready to define the ProcessingStep that will run the evaluation script:\n\nevaluate_model_step = ProcessingStep(\n    name=\"evaluate-model\",\n    step_args=evaluation_processor.run(\n        inputs=[\n            # The first input is the test split that we generated on\n            # the first step of the pipeline when we split and\n            # transformed the data.\n            ProcessingInput(\n                source=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n                    \"test\"\n                ].S3Output.S3Uri,\n                destination=\"/opt/ml/processing/test\",\n            ),\n            # The second input is the model that we generated on\n            # the Training or Tunning Step.\n            ProcessingInput(\n                source=model_assets,\n                destination=\"/opt/ml/processing/model\",\n            ),\n        ],\n        outputs=[\n            # The output is the evaluation report that we generated\n            # in the evaluation script.\n            ProcessingOutput(\n                output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"\n            ),\n        ],\n        code=f\"{CODE_FOLDER}/evaluation.py\",\n    ),\n    property_files=[evaluation_report],\n    cache_config=cache_config,\n)\n\n\n\nStep 3 - Registering the Model\nLet’s now create a new version of the model and register it in the Model Registry. Check Register a Model Version for more information about model registration.\nHere’s a high-level overview of how to register a model in the Model Registry:\n \nFirst, let’s define the name of the group where we’ll register the model:\n\nMODEL_PACKAGE_GROUP = \"penguins\"\n\nLet’s now create the model that we’ll register in the Model Registry. The model we trained uses TensorFlow, so we can use the built-in TensorFlowModel class to create an instance of the model:\n\nfrom sagemaker.tensorflow.model import TensorFlowModel\n\ntensorflow_model = TensorFlowModel(\n    model_data=model_assets,\n    framework_version=config[\"framework_version\"],\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\nWhen we register a model in the Model Registry, we can attach relevant metadata to it. We’ll use the evaluation report we generated during the Evaluation Step to populate the metrics of this model:\n\nfrom sagemaker.model_metrics import ModelMetrics, MetricsSource\nfrom sagemaker.workflow.functions import Join\n\nmodel_metrics = ModelMetrics(\n    model_statistics=MetricsSource(\n        s3_uri=Join(\n            on=\"/\",\n            values=[\n                evaluate_model_step.properties.ProcessingOutputConfig.Outputs[\n                    \"evaluation\"\n                ].S3Output.S3Uri,\n                \"evaluation.json\",\n            ],\n        ),\n        content_type=\"application/json\",\n    )\n)\n\nWe can use a Model Step to register the model. Check the ModelStep SageMaker’s SDK documentation for more information.\n\nfrom sagemaker.workflow.model_step import ModelStep\n\nregister_model_step = ModelStep(\n    name=\"register-model\",\n    step_args=tensorflow_model.register(\n        model_package_group_name=MODEL_PACKAGE_GROUP,\n        approval_status=\"Approved\",\n        model_metrics=model_metrics,\n        content_types=[\"text/csv\"],\n        response_types=[\"application/json\"],\n        inference_instances=[config[\"instance_type\"]],\n        transform_instances=[config[\"instance_type\"]],\n        domain=\"MACHINE_LEARNING\",\n        task=\"CLASSIFICATION\",\n        framework=\"TENSORFLOW\",\n        framework_version=config[\"framework_version\"],\n    ),\n)\n\n\n\nStep 4 - Setting up a Condition Step\nWe only want to register a new model if its accuracy exceeds a predefined threshold. We can use a Condition Step together with the evaluation report we generated to accomplish this.\nHere’s a high-level overview of the Condition Step:\n \nLet’s define a new Pipeline Parameter to specify the minimum accuracy that the model should reach for it to be registered.\n\nfrom sagemaker.workflow.parameters import ParameterFloat\n\naccuracy_threshold = ParameterFloat(name=\"accuracy_threshold\", default_value=0.70)\n\nIf the model’s accuracy is not greater than or equal our threshold, we will send the pipeline to a Fail Step with the appropriate error message. Check the FailStep SageMaker’s SDK documentation for more information.\n\nfrom sagemaker.workflow.fail_step import FailStep\n\nfail_step = FailStep(\n    name=\"fail\",\n    error_message=Join(\n        on=\" \",\n        values=[\n            \"Execution failed because the model's accuracy was lower than\",\n            accuracy_threshold,\n        ],\n    ),\n)\n\nWe can use a ConditionGreaterThanOrEqualTo condition to compare the model’s accuracy with the threshold. Look at the Conditions section in the documentation for more information about the types of supported conditions.\n\nfrom sagemaker.workflow.functions import JsonGet\nfrom sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n\ncondition = ConditionGreaterThanOrEqualTo(\n    left=JsonGet(\n        step_name=evaluate_model_step.name,\n        property_file=evaluation_report,\n        json_path=\"metrics.accuracy.value\",\n    ),\n    right=accuracy_threshold,\n)\n\nLet’s now define the Condition Step:\n\nfrom sagemaker.workflow.condition_step import ConditionStep\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[condition],\n    if_steps=[register_model_step] if not LOCAL_MODE else [],\n    else_steps=[fail_step],\n)\n\n\n\nStep 5 - Creating the Pipeline\nWe can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession3_pipeline = Pipeline(\n    name=\"session3-pipeline\",\n    parameters=[dataset_location, accuracy_threshold],\n    steps=[\n        preprocessing_step,\n        tune_model_step if USE_TUNING_STEP else train_model_step,\n        evaluate_model_step,\n        condition_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession3_pipeline.upsert(role_arn=role)\n\nWe can now start the pipeline:\n\nsession3_pipeline.start()\n\n\n\nAssignments\n\nAssignment 3.1 The evaluation script computes the accuracy of the model and exports it as part of the evaluation report. Extend the evaluation report by adding the precision and the recall of the model on each one of the classes.\nAssignment 3.2 Extend the evaluation script to test the model on each island separately. The evaluation report should contain the accuracy of the model on each island and the overall accuracy.\nAssignment 3.3 The Condition Step uses a hard-coded threshold value to determine if the model’s accuracy is good enough to proceed. Modify the code so the pipeline uses the accuracy of the latest registered model version as the threshold. We want to register a new model version only if its performance is better than the previous version we registered.\nAssignment 3.4 The current pipeline uses either a Training Step or a Tuning Step to build a model. Modify the pipeline to use both steps at the same time. The evaluation script should evaluate the model coming from the Training Step and the best model coming from the Tuning Step and output the accuracy and location in S3 of the best model. You should modify the code to register the model assets specified in the evaluation report.\nAssignment 3.5 Pipeline steps can encounter exceptions. In some cases, retrying can resolve these issues. For this assignment, configure the Processing Step so it automatically retries the step a maximum of 5 times if it encounters an InternalServerError. Check the Retry Policy for Pipeline Steps documentation for more information."
  },
  {
    "objectID": "cohort.html#session-4---deploying-models-and-serving-predictions",
    "href": "cohort.html#session-4---deploying-models-and-serving-predictions",
    "title": "Building Production Machine Learning Systems",
    "section": "Session 4 - Deploying Models and Serving Predictions",
    "text": "Session 4 - Deploying Models and Serving Predictions\nIn this session we’ll explore how to deploy a model to a SageMaker Endpoint and how to use a SageMaker Inference Pipeline to control the data that goes in and comes out of the endpoint.\n \nLet’s start by defining the name of the endpoint where we’ll deploy the model and creating a constant pointing to the location where we’ll store the data that the endpoint will capture:\n\nfrom sagemaker.predictor import Predictor\n\nENDPOINT = \"penguins-endpoint\"\nDATA_CAPTURE_DESTINATION = f\"{S3_LOCATION}/monitoring/data-capture\"\n\n\nStep 1 - Deploying Model From Registry\nLet’s manually deploy the latest model from the Model Registry to an endpoint.\nWe want to query the list of approved models from the Model Registry and get the last one:\n\nresponse = sagemaker_client.list_model_packages(\n    ModelPackageGroupName=MODEL_PACKAGE_GROUP,\n    ModelApprovalStatus=\"Approved\",\n    SortBy=\"CreationTime\",\n    MaxResults=1,\n)\n\npackage = (\n    response[\"ModelPackageSummaryList\"][0]\n    if response[\"ModelPackageSummaryList\"]\n    else None\n)\npackage\n\n{'ModelPackageGroupName': 'penguins',\n 'ModelPackageVersion': 77,\n 'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:325223348818:model-package/penguins/77',\n 'CreationTime': datetime.datetime(2023, 11, 3, 11, 30, 21, 903000, tzinfo=tzlocal()),\n 'ModelPackageStatus': 'Completed',\n 'ModelApprovalStatus': 'Approved'}\n\n\nWe can now create a Model Package using the ARN of the model from the Model Registry:\n\nfrom sagemaker import ModelPackage\n\nif package:\n    model_package = ModelPackage(\n        model_package_arn=package[\"ModelPackageArn\"],\n        sagemaker_session=sagemaker_session,\n        role=role,\n    )\n\nLet’s now deploy the model to an endpoint.\nHere is an overview of the three components of an Endpoint:\n \n\nmodel_package.deploy(\n    endpoint_name=ENDPOINT, \n    initial_instance_count=1, \n    instance_type=config[\"instance_type\"]\n)\n\nAfter deploying the model, we can test the endpoint to make sure it works.\nEach line of the payload we’ll send to the endpoint contains the information of a penguin. Notice the model expects data that’s already transformed. We can’t provide the original data from our dataset because the model we registered will not work with it.\nThe endpoint will return the predictions for each of these lines.\n\npayload = \"\"\"\n0.6569590202313976,-1.0813829646495108,1.2097102831892812,0.9226343641317372,1.0,0.0,0.0\n-0.7751048801481084,0.8822689351285553,-1.2168066120762704,0.9226343641317372,0.0,1.0,0.0\n-0.837387834894918,0.3386660813829646,-0.26237731892812,-1.92351941317372,0.0,0.0,1.0\n\"\"\"\n\nLet’s send the payload to the endpoint and print its response:\n\npredictor = Predictor(endpoint_name=ENDPOINT)\n\ntry:\n    response = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n    response = json.loads(response.decode(\"utf-8\"))\n\n    print(json.dumps(response, indent=2))\n    print(f\"\\nSpecies: {np.argmax(response['predictions'], axis=1)}\")\nexcept Exception as e:\n    print(e)\n\nAn error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint penguins-endpoint of account 325223348818 not found.\n\n\nAfter testing the endpoint, we need to ensure we delete it:\n\npredictor.delete_endpoint()\n\nDeploying the model we trained directly to an endpoint doesn’t lets us control the data that goes in and comes out of the endpoint. The TensorFlow model we trained requires transformed data, which makes it useless to other applications:\n \nFortunately, we can create an Inference Pipeline using SageMaker to control the data that goes in and comes out of the endpoint.\nOur inference pipeline will have three components:\n\nA preprocessing transformer that will transform the input data into the format the model expects.\nThe TensorFlow model we trained.\nA postprocessing transformer that will transform the output of the model into a human-readable format.\n\n \nWe want our endpoint to handle unprocessed data in CSV and JSON format and return the penguin’s species. Here is an example of the payload input we want the endpoint to support:\n{\n    \"island\": \"Biscoe\",\n    \"culmen_length_mm\": 48.6,\n    \"culmen_depth_mm\": 16.0,\n    \"flipper_length_mm\": 230.0,\n    \"body_mass_g\": 5800.0,\n}\nAnd here is an example of the output we’d like to get from the endpoint:\n{\n    \"prediction\": \"Adelie\",\n    \"confidence\": 0.802672\n}\n\n\nStep 2 - Creating the Preprocessing Script\nThe first component of our inference pipeline will transform the input data into the format the model expects. We’ll use the Scikit-Learn transformer we saved when we split and transformed the data. To deploy this component as part of an inference pipeline, we need to write a script that loads the transformer, uses it to modify the input data, and returns the output in the format the TensorFlow model expects.\n\n\n\npreprocessing_component.py\n\nimport os\nimport pandas as pd\nimport json\nimport joblib\n\nfrom io import StringIO\n\ntry:\n    from sagemaker_containers.beta.framework import encoders, worker\nexcept ImportError:\n    # We don't have access to the `worker` instance when testing locally.\n    # We'll set it to None so we can change the way functions create\n    # a response.\n    worker = None\n\n\nTARGET_COLUMN = \"species\"\nFEATURE_COLUMNS = [\n    \"island\",\n    \"culmen_length_mm\",\n    \"culmen_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\",\n    \"sex\",\n]\n\n\ndef model_fn(model_dir):\n    \"\"\"\n    Deserializes the model that will be used in this container.\n    \"\"\"\n\n    return joblib.load(os.path.join(model_dir, \"features.joblib\"))\n    \n\ndef input_fn(input_data, content_type):\n    \"\"\"\n    Parses the input payload and creates a Pandas DataFrame.\n\n    This function will check whether the target column is present in the\n    input data, and will remove it.\n    \"\"\"\n\n    if content_type == \"text/csv\":\n        df = pd.read_csv(StringIO(input_data), header=None, skipinitialspace=True)\n\n        if len(df.columns) == len(FEATURE_COLUMNS) + 1:\n            df = df.drop(df.columns[0], axis=1)\n\n        df.columns = FEATURE_COLUMNS\n        return df\n\n    if content_type == \"application/json\":\n        df = pd.DataFrame([json.loads(input_data)])\n\n        if \"species\" in df.columns:\n            df = df.drop(\"species\", axis=1)\n\n        return df\n\n    raise ValueError(f\"{content_type} is not supported!\")\n\n\ndef predict_fn(input_data, model):\n    \"\"\"\n    Preprocess the input using the transformer.\n    \"\"\"\n\n    try:\n        response = model.transform(input_data)\n        return response\n    except ValueError as e:\n        print(\"Error transforming the input data\", e)\n        return None\n\n\ndef output_fn(prediction, accept):\n    \"\"\"\n    Formats the prediction output to generate a response.\n\n    The default accept/content-type between containers for serial inference\n    is JSON. Since this model will preceed a TensorFlow model, we want to\n    return a JSON object following TensorFlow's input requirements.\n    \"\"\"\n\n    if prediction is None:\n        raise Exception(\"There was an error transforming the input data\")\n\n    instances = [p for p in prediction.tolist()]\n    response = {\"instances\": instances}\n    return (\n        worker.Response(json.dumps(response), mimetype=accept)\n        if worker\n        else (response, accept)\n    )\n\n    raise Exception(f\"{accept} accept type is not supported.\")\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nfrom preprocessing_component import input_fn, predict_fn, output_fn, model_fn\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    \n    preprocess(base_directory=directory)\n    \n    with tarfile.open(directory / \"model\" / \"model.tar.gz\") as tar:\n        tar.extractall(path=directory / \"model\")\n    \n    yield directory / \"model\"\n    \n    shutil.rmtree(directory)\n\n\n\ndef test_input_csv_drops_target_column_if_present():\n    input_data = \"\"\"\n    Adelie, Torgersen, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    df = input_fn(input_data, \"text/csv\")\n    assert len(df.columns) == 6 and \"species\" not in df.columns\n\n\ndef test_input_json_drops_target_column_if_present():\n    input_data = json.dumps({\n        \"species\": \"Adelie\", \n        \"island\": \"Torgersen\",\n        \"culmen_length_mm\": 44.1,\n        \"culmen_depth_mm\": 18.0,\n        \"flipper_length_mm\": 210.0,\n        \"body_mass_g\": 4000.0,\n        \"sex\": \"MALE\"\n    })\n    \n    df = input_fn(input_data, \"application/json\")\n    assert len(df.columns) == 6 and \"species\" not in df.columns\n\n\ndef test_input_csv_works_without_target_column():\n    input_data = \"\"\"\n    Torgersen, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    df = input_fn(input_data, \"text/csv\")\n    assert len(df.columns) == 6\n\n\ndef test_input_json_works_without_target_column():\n    input_data = json.dumps({\n        \"island\": \"Torgersen\",\n        \"culmen_length_mm\": 44.1,\n        \"culmen_depth_mm\": 18.0,\n        \"flipper_length_mm\": 210.0,\n        \"body_mass_g\": 4000.0,\n        \"sex\": \"MALE\"\n    })\n    \n    df = input_fn(input_data, \"application/json\")\n    assert len(df.columns) == 6\n\n\ndef test_output_raises_exception_if_prediction_is_none():\n    with pytest.raises(Exception):\n        output_fn(None, \"application/json\")\n    \n    \ndef test_output_returns_tensorflow_ready_input():\n    prediction = np.array([\n        [-1.3944109908736013,1.15488062669371,-0.7954340636549508,-0.5536447804097907,0.0,1.0,0.0],\n        [1.0557485835338234,0.5040085971987002,-0.5824506029515057,-0.5851840035995248,0.0,1.0,0.0]\n    ])\n    \n    response = output_fn(prediction, \"application/json\")\n    \n    assert response[0] == {\n        \"instances\": [\n            [-1.3944109908736013,1.15488062669371,-0.7954340636549508,-0.5536447804097907,0.0,1.0,0.0],\n            [1.0557485835338234,0.5040085971987002,-0.5824506029515057,-0.5851840035995248,0.0,1.0,0.0]\n        ]\n    }\n    \n    assert response[1] == \"application/json\"\n\n    \ndef test_predict_transforms_data(directory):\n    input_data = \"\"\"\n    Torgersen, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    model = model_fn(str(directory))\n    df = input_fn(input_data, \"text/csv\")\n    response = predict_fn(df, model)\n    assert type(response) is np.ndarray\n    \n\ndef test_predict_returns_none_if_invalid_input(directory):\n    input_data = \"\"\"\n    Invalid, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    model = model_fn(str(directory))\n    df = input_fn(input_data, \"text/csv\")\n    assert predict_fn(df, model) is None\n\n\n\n\nStep 3 - Creating the Postprocessing Script\nThe final component of our inference pipeline will transform the output from the model into a human-readable format. We’ll use the Scikit-Learn target transformer we saved when we split and transformed the data. To deploy this component as part of an inference pipeline, we need to write a script that loads the transformer, uses it to modify the output from the model, and returns a human-readable format.\n\n\n\npostprocessing_component.py\n\nimport os\nimport numpy as np\nimport json\nimport joblib\n\n\ntry:\n    from sagemaker_containers.beta.framework import encoders, worker\nexcept ImportError:\n    # We don't have access to the `worker` instance when testing locally.\n    # We'll set it to None so we can change the way functions create\n    # a response.\n    worker = None\n\n\ndef model_fn(model_dir):\n    \"\"\"\n    Deserializes the target model and returns the list of fitted categories.\n    \"\"\"\n\n    model = joblib.load(os.path.join(model_dir, \"target.joblib\"))\n    return model.named_transformers_[\"species\"].categories_[0]\n\n\ndef input_fn(input_data, content_type):\n    if content_type == \"application/json\":\n        predictions = json.loads(input_data)[\"predictions\"]\n        return predictions\n    \n    raise ValueError(f\"{content_type} is not supported.!\")\n\n\ndef predict_fn(input_data, model):\n    \"\"\"\n    Transforms the prediction into its corresponding category.\n    \"\"\"\n    predictions = np.argmax(input_data, axis=-1)\n    confidence = np.max(input_data, axis=-1)\n    return [\n        (model[prediction], confidence)\n        for confidence, prediction in zip(confidence, predictions)\n    ]\n\ndef output_fn(prediction, accept):\n    if accept == \"text/csv\":\n        return (\n            worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n            if worker\n            else (prediction, accept)\n        )\n\n    if accept == \"application/json\":\n        response = []\n        for p, c in prediction:\n            response.append({\"prediction\": p, \"confidence\": c})\n\n        # If there's only one prediction, we'll return it\n        # as a single object.\n        if len(response) == 1:\n            response = response[0]\n\n        return (\n            worker.Response(json.dumps(response), mimetype=accept)\n            if worker\n            else (response, accept)\n        )\n\n    raise Exception(f\"{accept} accept type is not supported.\")\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nimport numpy as np\n\nfrom postprocessing_component import predict_fn, output_fn\n\n\ndef test_predict_returns_prediction_as_first_column():\n    input_data = [\n        [0.6, 0.2, 0.2], \n        [0.1, 0.8, 0.1],\n        [0.2, 0.1, 0.7]\n    ]\n    \n    categories = [\"Adelie\", \"Gentoo\", \"Chinstrap\"]\n    \n    response = predict_fn(input_data, categories)\n    \n    assert response == [\n        (\"Adelie\", 0.6),\n        (\"Gentoo\", 0.8),\n        (\"Chinstrap\", 0.7)\n    ]\n\n\ndef test_output_does_not_return_array_if_single_prediction():\n    prediction = [(\"Adelie\", 0.6)]\n    response, _ = output_fn(prediction, \"application/json\")\n\n    assert response[\"prediction\"] == \"Adelie\"\n\n\ndef test_output_returns_array_if_multiple_predictions():\n    prediction = [(\"Adelie\", 0.6), (\"Gentoo\", 0.8)]\n    response, _ = output_fn(prediction, \"application/json\")\n\n    assert len(response) == 2\n    assert response[0][\"prediction\"] == \"Adelie\"\n    assert response[1][\"prediction\"] == \"Gentoo\"\n\n\n\n\nStep 4 - Setting up the Inference Pipeline\nWe can now create a PipelineModel to define our inference pipeline.\nWe’ll use the model we generated from the first step of the pipeline as the input to the first and last components of the inference pipeline. This model.tar.gz file contains the two transformers we need to preprocess and postprocess the data. Let’s create a variable with the URI to this file:\n\ntransformation_pipeline_model = Join(\n    on=\"/\",\n    values=[\n        preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n            \"model\"\n        ].S3Output.S3Uri,\n        \"model.tar.gz\",\n    ],\n)\n\nHere is the first component of the inference pipeline. It will preprocess the data before sending it to the TensorFlow model:\n\nfrom sagemaker.sklearn.model import SKLearnModel\n\npreprocessing_model = SKLearnModel(\n    model_data=transformation_pipeline_model,\n    entry_point=\"preprocessing_component.py\",\n    source_dir=str(INFERENCE_CODE_FOLDER),\n    framework_version=\"1.2-1\",\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\nHere is the last component of the inference pipeline. It will postprocess the output from the TensorFlow model before sending it back to the user:\n\npost_processing_model = SKLearnModel(\n    model_data=transformation_pipeline_model,\n    entry_point=\"postprocessing_component.py\",\n    source_dir=str(INFERENCE_CODE_FOLDER),\n    framework_version=\"1.2-1\",\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\nWe can now create the inference pipeline using the three models:\n\nfrom sagemaker.pipeline import PipelineModel\n\npipeline_model = PipelineModel(\n    name=\"inference-model\",\n    models=[preprocessing_model, tensorflow_model, post_processing_model],\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\n\n\nStep 5 - Registering the Model\nWe’ll modify the pipeline to register the Pipeline Model in the Model Registry. We’ll use a different group name to keep Pipeline Models separate.\n\nPIPELINE_MODEL_PACKAGE_GROUP = \"pipeline\"\n\nLet’s now register the model. Notice that we will register the model with “PendingManualApproval” status. This means that we’ll need to manually approve the model before it can be deployed to an endpoint. Check Register a Model Version for more information about model registration.\n\nregister_model_step = ModelStep(\n    name=\"register\",\n    display_name=\"register-model\",\n    step_args=pipeline_model.register(\n        model_package_group_name=PIPELINE_MODEL_PACKAGE_GROUP,\n        model_metrics=model_metrics,\n        approval_status=\"PendingManualApproval\",\n        content_types=[\"text/csv\", \"application/json\"],\n        response_types=[\"text/csv\", \"application/json\"],\n        inference_instances=[config[\"instance_type\"]],\n        transform_instances=[config[\"instance_type\"]],\n        domain=\"MACHINE_LEARNING\",\n        task=\"CLASSIFICATION\",\n        framework=\"TENSORFLOW\",\n        framework_version=config[\"framework_version\"],\n    ),\n)\n\n\n\nStep 6 - Modifying the Condition Step\nSince we modified the registration step, we also need to modify the Condition Step to use the new registration:\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[condition],\n    if_steps=[register_model_step] if not LOCAL_MODE else [],\n    else_steps=[fail_step],\n)\n\n\n\nStep 7 - Creating the Pipeline\nWe can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession4_pipeline = Pipeline(\n    name=\"session4-pipeline\",\n    parameters=[dataset_location, accuracy_threshold],\n    steps=[\n        preprocessing_step,\n        tune_model_step if USE_TUNING_STEP else train_model_step,\n        evaluate_model_step,\n        condition_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession4_pipeline.upsert(role_arn=role)\n\nWe can now start the pipeline:\n\nsession4_pipeline.start()\n\n\n\nStep 8 - Creating the Lambda Function\nWe will use Amazon EventBridge to trigger a Lambda function that will deploy the model whenever its status changes from “PendingManualApproval” to “Approved.” Let’s start by writing the Lambda function to take the model information and create a new endpoint.\nWe’ll enable Data Capture as part of the endpoint configuration. With Data Capture we can record the inputs and outputs of the endpoint to use them later for monitoring the model:\n\nInitialSamplingPercentage represents the percentage of traffic that we want to capture.\nDestinationS3Uri specifies the S3 location where we want to store the captured data.\n\n\nimport os\nimport json\nimport boto3\nimport time\n\nsagemaker = boto3.client(\"sagemaker\")\n\ndef lambda_handler(event, context):\n    model_package_arn = event[\"detail\"][\"ModelPackageArn\"]\n    approval_status = event[\"detail\"][\"ModelApprovalStatus\"]\n\n    print(f\"Model: {model_package_arn}\")\n    print(f\"Approval status: {approval_status}\")\n    \n    # We only want to deploy the approved models\n    if approval_status != \"Approved\":\n        response = {\n            \"message\": \"Skipping deployment.\",\n            \"approval_status\": approval_status,\n        }\n\n        print(response)\n        return {\n            \"statusCode\": 200,\n            \"body\": json.dumps(response)\n        }    \n    \n    endpoint_name = os.environ[\"ENDPOINT\"]\n    data_capture_destination = os.environ[\"DATA_CAPTURE_DESTINATION\"]\n    role = os.environ[\"ROLE\"]\n    \n    timestamp = time.strftime(\"%m%d%H%M%S\", time.localtime())\n    model_name = f\"{endpoint_name}-model-{timestamp}\"\n    endpoint_config_name = f\"{endpoint_name}-config-{timestamp}\"\n\n    sagemaker.create_model(\n        ModelName=model_name, \n        ExecutionRoleArn=role, \n        Containers=[{\n            \"ModelPackageName\": model_package_arn\n        }] \n    )\n\n    sagemaker.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[{\n            \"ModelName\": model_name,\n            \"InstanceType\": \"ml.m5.xlarge\",\n            \"InitialVariantWeight\": 1,\n            \"InitialInstanceCount\": 1,\n            \"VariantName\": \"AllTraffic\",\n        }],\n\n        # We can enable Data Capture to record the inputs and outputs\n        # of the endpoint to use them later for monitoring the model. \n        DataCaptureConfig={\n            \"EnableCapture\": True,\n            \"InitialSamplingPercentage\": 100,\n            \"DestinationS3Uri\": data_capture_destination,\n            \"CaptureOptions\": [\n                {\n                    \"CaptureMode\": \"Input\"\n                },\n                {\n                    \"CaptureMode\": \"Output\"\n                },\n            ],\n            \"CaptureContentTypeHeader\": {\n                \"CsvContentTypes\": [\n                    \"text/csv\",\n                    \"application/octect-stream\"\n                ],\n                \"JsonContentTypes\": [\n                    \"application/json\",\n                    \"application/octect-stream\"\n                ]\n            }\n        },\n    )\n    \n    response = sagemaker.list_endpoints(NameContains=endpoint_name, MaxResults=1)\n\n    if len(response[\"Endpoints\"]) == 0:\n        # If the endpoint doesn't exist, let's create it.\n        sagemaker.create_endpoint(\n            EndpointName=endpoint_name, \n            EndpointConfigName=endpoint_config_name,\n        )\n    else:\n        # If the endpoint already exist, let's update it with the\n        # new configuration.\n        sagemaker.update_endpoint(\n            EndpointName=endpoint_name, \n            EndpointConfigName=endpoint_config_name,\n        )\n    \n    return {\n        \"statusCode\": 200,\n        \"body\": json.dumps(\"Endpoint deployed successfully\")\n    }\n\nOverwriting code/lambda.py\n\n\nWe need to ensure our Lambda function has permission to interact with SageMaker, so let’s create a new role and then create the lambda function.\n\nlambda_role_name = \"lambda-deployment-role\"\nlambda_role_arn = None\n\ntry:\n    response = iam_client.create_role(\n        RoleName=lambda_role_name,\n        AssumeRolePolicyDocument=json.dumps(\n            {\n                \"Version\": \"2012-10-17\",\n                \"Statement\": [\n                    {\n                        \"Effect\": \"Allow\",\n                        \"Principal\": {\n                            \"Service\": [\"lambda.amazonaws.com\", \"events.amazonaws.com\"]\n                        },\n                        \"Action\": \"sts:AssumeRole\",\n                    }\n                ],\n            }\n        ),\n        Description=\"Lambda Endpoint Deployment\",\n    )\n\n    lambda_role_arn = response[\"Role\"][\"Arn\"]\n\n    iam_client.attach_role_policy(\n        PolicyArn=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\",\n        RoleName=lambda_role_name,\n    )\n\n    iam_client.attach_role_policy(\n        PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\",\n        RoleName=lambda_role_name,\n    )\n\n    print(f'Role \"{lambda_role_name}\" created with ARN \"{lambda_role_arn}\".')\nexcept iam_client.exceptions.EntityAlreadyExistsException:\n    response = iam_client.get_role(RoleName=lambda_role_name)\n    lambda_role_arn = response[\"Role\"][\"Arn\"]\n    print(f'Role \"{lambda_role_name}\" already exists with ARN \"{lambda_role_arn}\".')\n\nWe can now create the Lambda function:\n\nfrom sagemaker.lambda_helper import Lambda\n\n\ndeploy_lambda_fn = Lambda(\n    function_name=\"deploy_fn\",\n    execution_role_arn=lambda_role_arn,\n    script=str(CODE_FOLDER / \"lambda.py\"),\n    handler=\"lambda.lambda_handler\",\n    timeout=600,\n    session=sagemaker_session,\n    runtime=\"python3.11\",\n    environment={\n        \"Variables\": {\n            \"ENDPOINT\": ENDPOINT,\n            \"DATA_CAPTURE_DESTINATION\": DATA_CAPTURE_DESTINATION,\n            \"ROLE\": role,\n        }\n    },\n)\n\nlambda_response = deploy_lambda_fn.upsert()\nlambda_response\n\n\n\nStep 9 - Setting Up EventBridge\nWe can now create an EventBridge rule that triggers the deployment process whenever a model approval status becomes “Approved”. To do this, let’s define the event pattern that will trigger the deployment process:\n\nevent_pattern = f\"\"\"\n{{\n  \"source\": [\"aws.sagemaker\"],\n  \"detail-type\": [\"SageMaker Model Package State Change\"],\n  \"detail\": {{\n    \"ModelPackageGroupName\": [\"{PIPELINE_MODEL_PACKAGE_GROUP}\"],\n    \"ModelApprovalStatus\": [\"Approved\"]\n  }}\n}}\n\"\"\"\n\nLet’s now create the EventBridge rule:\n\nevents_client = boto3.client(\"events\")\nrule_response = events_client.put_rule(\n    Name=\"PipelineModelApprovedRule\",\n    EventPattern=event_pattern,\n    State=\"ENABLED\",\n    RoleArn=role,\n)\n\nNow, we need to define the target of the rule. The target will trigger whenever the rule matches an event. In this case, we want to trigger the Lambda function we created before:\n\nresponse = events_client.put_targets(\n    Rule=\"PipelineModelApprovedRule\",\n    Targets=[\n        {\n            \"Id\": \"1\",\n            \"Arn\": lambda_response[\"FunctionArn\"],\n        }\n    ],\n)\n\nFinally, we need to give the Lambda function permission to be triggered by the EventBridge rule:\n\nlambda_client = boto3.client(\"lambda\")\ntry:\n    response = lambda_client.add_permission(\n        Action=\"lambda:InvokeFunction\",\n        FunctionName=lambda_response[\"FunctionName\"],\n        Principal=\"events.amazonaws.com\",\n        SourceArn=rule_response[\"RuleArn\"],\n        StatementId=\"EventBridge\",\n    )\nexcept lambda_client.exceptions.ResourceConflictException as e:\n    print(f'Function \"{lambda_response[\"FunctionName\"]}\" already has permissions.')\n\nFunction \"deploy_fn\" already has permissions.\n\n\n\n\nStep 10 - Testing the Endpoint\nLet’s now test the endpoint we deployed automatically with the pipeline. We will use the function to create a predictor with a JSON encoder and decoder.\n\nfrom sagemaker.serializers import CSVSerializer\n\npredictor = Predictor(\n    endpoint_name=ENDPOINT, \n    serializer=CSVSerializer(),\n    sagemaker_session=sagemaker_session\n)\n\ndata = pd.read_csv(DATA_FILEPATH)\ndata = data.drop(\"species\", axis=1)\n\npayload = data.iloc[:3].to_csv(header=False, index=False)\nprint(f\"Payload:\\n{payload}\")\n\ntry:\n    response = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n    response = json.loads(response.decode(\"utf-8\"))\n    print(json.dumps(response, indent=2))\nexcept Exception as e:\n    print(e)\n\nPayload:\nTorgersen,39.1,18.7,181.0,3750.0,MALE\nTorgersen,39.5,17.4,186.0,3800.0,FEMALE\nTorgersen,40.3,18.0,195.0,3250.0,FEMALE\n\nAn error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint penguins-endpoint of account 325223348818 not found.\n\n\nLet’s now test the endpoint by sending a JSON payload:\n\nfrom sagemaker.serializers import JSONSerializer\n\nsample = {\n    \"island\": \"Biscoe\",\n    \"culmen_length_mm\": 48.6,\n    \"culmen_depth_mm\": 16.0,\n    \"flipper_length_mm\": 230.0,\n    \"body_mass_g\": 5800.0,\n    \"sex\": \"MALE\"\n}\n\npredictor = Predictor(\n    endpoint_name=ENDPOINT, \n    serializer=JSONSerializer(),\n    sagemaker_session=sagemaker_session\n)\n\ntry:\n    response = predictor.predict(sample)\n    print(json.loads(response.decode(\"utf-8\")))\nexcept Exception as e:\n    print(e)\n\nAn error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint penguins-endpoint of account 325223348818 not found.\n\n\nAnd now let’s send the same payload but return the prediction in CSV format:\n\ntry:\n    response = predictor.predict(sample, initial_args={\"Accept\": \"text/csv\"})\n    print(response.decode(\"utf-8\"))\nexcept Exception as e:\n    print(e)\n\nAn error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint penguins-endpoint of account 325223348818 not found.\n\n\nLet’s delete the endpoint:\n\npredictor.delete_endpoint()\n\n\n\nAssignments\n\nAssignment 4.1 Every Endpoint has an invocation URL you can use to generate predictions with the model from outside AWS. As part of this assignment, write a simple Python script that will run on your local computer and run a few samples through the Endpoint. You will need your AWS access key and secret to connect to the Endpoint.\nAssignment 4.2 We can use model variants to perform A/B testing between a new model and an old model. Create a function that given the ARN of two models in the Model Registry deploys them to an Endpoint as separate variants. Each variant should receive 50% of the traffic. Write another function that invokes the endpoint by default, but allows the caller to invoke a specific variant if they want to.\nAssignment 4.3 We can use SageMaker Model Shadow Deployments to create shadow variants to validate a new model version before promoting it to production. Write a function that given the ARN of a model in the Model Registry, updates an Endpoint and deploys the model as a shadow variant. Check Shadow variants for more information about this topic. Send some traffic to the Endpoint and compare the results from the main model with its shadow variant.\nAssignment 4.4 SageMaker supports auto scaling your models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in the workload. For this assignment, define a target-tracking scaling policy for a variant of your Endpoint and use the SageMakerVariantInvocationsPerInstance metric. SageMakerVariantInvocationsPerInstance is the average number of times per minute that the variant is invoked. Check Automatically Scale Amazon SageMaker Models for more information about auto scaling models.\nAssignment 4.5 Modify the SageMaker Pipeline by adding a Lambda Step that will deploy the model directly as part of the pipeline. You won’t need to set up Event Bridge anymore because your pipeline will automatically deploy the model."
  },
  {
    "objectID": "cohort.html#session-5---data-distribution-shifts-and-model-monitoring",
    "href": "cohort.html#session-5---data-distribution-shifts-and-model-monitoring",
    "title": "Building Production Machine Learning Systems",
    "section": "Session 5 - Data Distribution Shifts And Model Monitoring",
    "text": "Session 5 - Data Distribution Shifts And Model Monitoring\nIn this session we’ll set up a monitoring process to analyze the quality of the data our endpoint receives and the endpoint predictions. For this, we need to check the data received by the endpoint, generate ground truth labels, and compare them with a baseline performance.\n \nTo enable this functionality, we need a couple of steps:\n\nCreate baselines we can use to compare against real-time traffic.\nSet up a schedule to continuously evaluate and compare against the baselines.\n\nCheck Amazon SageMaker Model Monitor for a brief explanation of how to use SageMaker’s Model Monitoring functionality. Monitor models for data and model quality, bias, and explainability is a much more extensive guide to monitoring in Amazon SageMaker.\nLet’s start by defining three variables we’ll use throughout the session:\n\nGROUND_TRUTH_LOCATION = f\"{S3_LOCATION}/monitoring/groundtruth\"\nDATA_QUALITY_LOCATION = f\"{S3_LOCATION}/monitoring/data-quality\"\nMODEL_QUALITY_LOCATION = f\"{S3_LOCATION}/monitoring/model-quality\"\n\n\nStep 1 - Generating Data Quality Baseline\nLet’s start by configuring a Quality Check Step to compute the general statistics of the data we used to build our model.\nWe can configure the instance that will run the quality check using the CheckJobConfig class, and we can use the DataQualityCheckConfig class to configure the job.\n\nfrom sagemaker.workflow.quality_check_step import (\n    QualityCheckStep,\n    DataQualityCheckConfig,\n)\nfrom sagemaker.workflow.check_job_config import CheckJobConfig\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\n\ndata_quality_baseline_step = QualityCheckStep(\n    name=\"generate-data-quality-baseline\",\n    check_job_config=CheckJobConfig(\n        instance_type=\"ml.c5.xlarge\",\n        instance_count=1,\n        volume_size_in_gb=20,\n        sagemaker_session=pipeline_session,\n        role=role,\n    ),\n    quality_check_config=DataQualityCheckConfig(\n        baseline_dataset=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n            \"train-baseline\"\n        ].S3Output.S3Uri,\n        dataset_format=DatasetFormat.csv(header=True, output_columns_position=\"START\"),\n        output_s3_uri=DATA_QUALITY_LOCATION,\n    ),\n    model_package_group_name=PIPELINE_MODEL_PACKAGE_GROUP,\n    skip_check=True,\n    register_new_baseline=True,\n    cache_config=cache_config,\n)\n\n\n\nStep 2 - Generating Test Predictions\nTo create a baseline to compare the model performance, we must create predictions for the test set and compare the model’s metrics with the model performance on production data. We can do this by running a Batch Transform Job to predict every sample from the test set. We can use a Transform Step as part of the pipeline to run this job. This Batch Transform Job will run every sample from the training dataset through the model so we can compute the baseline metrics.\nThe Transform Step requires a model to generate predictions, so we need a Model Step that creates a model:\n\nfrom sagemaker.workflow.model_step import ModelStep\n\ncreate_model_step = ModelStep(\n    name=\"create-model\",\n    step_args=pipeline_model.create(instance_type=config[\"instance_type\"]),\n)\n\nLet’s configure the Batch Transform Job using an instance of the Transformer class:\n\nfrom sagemaker.transformer import Transformer\n\ntransformer = Transformer(\n    model_name=create_model_step.properties.ModelName,\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    strategy=\"MultiRecord\",\n    accept=\"text/csv\",\n    assemble_with=\"Line\",\n    output_path=f\"{S3_LOCATION}/transform\",\n    sagemaker_session=config[\"session\"],\n)\n\nWe can now set up the Transform Step using the transformer we configured before.\nNotice the following:\n\nWe’ll generate predictions for the baseline output that we generated when we split and transformed the data. This baseline is the same data we used to test the model, but we saved it in its original format before transforming it.\nThe output of this Batch Transform Job will have two fields. The first one will be the ground truth label, and the second one will be the prediction of the model.\n\n\nfrom sagemaker.workflow.steps import TransformStep\n\ngenerate_test_predictions_step = (\n    TransformStep(\n        name=\"generate-test-predictions\",\n        step_args=transformer.transform(\n            # We will use the baseline set we generated when we split the data.\n            # This set corresponds to the test split before the transformation step.\n            data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n                \"test-baseline\"\n            ].S3Output.S3Uri,\n            join_source=\"Input\",\n            split_type=\"Line\",\n            content_type=\"text/csv\",\n            # We want to output the first and the second to last field from\n            # the joint set. The first field corresponds to the groundtruth,\n            # and the second to last field corresponds to the prediction.\n            #\n            # Here is an example of the data the Transform Job will generate\n            # after joining the input with the output from the model:\n            #\n            # Gentoo,39.1,18.7,181.0,3750.0,MALE,Gentoo,0.52\n            #\n            # Notice how the first field is the groundtruth coming from the\n            # test set. The second to last field is the prediction coming the\n            # model.\n            output_filter=\"$[0,-2]\",\n        ),\n        cache_config=cache_config,\n    )\n)\n\n\n\nStep 3 - Generating Model Quality Baseline\nLet’s now configure the Quality Check Step and feed it the data we generated in the Transform Step. This step will automatically compute the performance metrics of the model on the test set:\n\nfrom sagemaker.workflow.quality_check_step import ModelQualityCheckConfig\n\nmodel_quality_baseline_step = QualityCheckStep(\n    name=\"generate-model-quality-baseline\",\n    check_job_config=CheckJobConfig(\n        instance_type=\"ml.c5.xlarge\",\n        instance_count=1,\n        volume_size_in_gb=20,\n        sagemaker_session=pipeline_session,\n        role=role,\n    ),\n    quality_check_config=ModelQualityCheckConfig(\n        # We are going to use the output of the Transform Step to generate\n        # the model quality baseline.\n        baseline_dataset=generate_test_predictions_step.properties.TransformOutput.S3OutputPath,\n        dataset_format=DatasetFormat.csv(header=False),\n\n        # We need to specify the problem type and the fields where the prediction\n        # and groundtruth are so the process knows how to interpret the results.\n        problem_type=\"MulticlassClassification\",\n        \n        # Since the data doesn't have headers, SageMaker will autocreate headers for it.\n        # _c0 corresponds to the first column, and _c1 corresponds to the second column.\n        ground_truth_attribute=\"_c0\",\n        inference_attribute=\"_c1\",\n        output_s3_uri=MODEL_QUALITY_LOCATION,\n    ),\n    model_package_group_name=PIPELINE_MODEL_PACKAGE_GROUP,\n    skip_check=True,\n    register_new_baseline=True,\n    cache_config=cache_config,\n)\n\n\n\nStep 4 - Setting up Model Metrics\nWe can configure a new set of ModelMetrics using the results of the Data and Model Quality Steps. Check Baseline and model version lifecycle and evolution with SageMaker Pipelines for an explanation of how SageMaker uses the DriftCheckBaselines.\n\nfrom sagemaker.drift_check_baselines import DriftCheckBaselines\n\nmodel_metrics = ModelMetrics(\n    model_data_statistics=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineStatistics,\n        content_type=\"application/json\",\n    ),\n    model_data_constraints=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineConstraints,\n        content_type=\"application/json\",\n    ),\n    model_statistics=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.CalculatedBaselineStatistics,\n        content_type=\"application/json\",\n    ),\n    model_constraints=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.CalculatedBaselineConstraints,\n        content_type=\"application/json\",\n    ),\n)\n\ndrift_check_baselines = DriftCheckBaselines(\n    model_data_statistics=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckStatistics,\n        content_type=\"application/json\",\n    ),\n    model_data_constraints=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckConstraints,\n        content_type=\"application/json\",\n    ),\n    model_statistics=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.BaselineUsedForDriftCheckStatistics,\n        content_type=\"application/json\",\n    ),\n    model_constraints=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.BaselineUsedForDriftCheckConstraints,\n        content_type=\"application/json\",\n    ),\n)\n\n\n\nStep 5 - Modifying the Registration Step\nSince we want to register the model using the new metrics, we need to modify the Registration Step to use the new metrics:\n\nregister_model_step = ModelStep(\n    name=\"register\",\n    display_name=\"register-model\",\n    step_args=pipeline_model.register(\n        model_package_group_name=PIPELINE_MODEL_PACKAGE_GROUP,\n        model_metrics=model_metrics,\n        drift_check_baselines=drift_check_baselines,\n        approval_status=\"PendingManualApproval\",\n        content_types=[\"text/csv\", \"application/json\"],\n        response_types=[\"text/csv\", \"application/json\"],\n        inference_instances=[config[\"instance_type\"]],\n        transform_instances=[config[\"instance_type\"]],\n        domain=\"MACHINE_LEARNING\",\n        task=\"CLASSIFICATION\",\n        framework=\"TENSORFLOW\",\n        framework_version=config[\"framework_version\"],\n    ),\n)\n\n\n\nStep 6 - Modifying the Condition Step\nSince we modified the registration step and added a few more steps, we need to modify the Condition Step. Now, we want to generate the test predictions and compute the model quality baseline if the condition is successful:\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[condition],\n    if_steps=[\n        create_model_step,\n        generate_test_predictions_step,\n        model_quality_baseline_step,\n        register_model_step,\n    ]\n    if not LOCAL_MODE\n    else [],\n    else_steps=[fail_step],\n)\n\n\n\nStep 7 - Creating the Pipeline\nWe can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession5_pipeline = Pipeline(\n    name=\"session5-pipeline\",\n    parameters=[dataset_location, accuracy_threshold],\n    steps=[\n        preprocessing_step,\n        tune_model_step if USE_TUNING_STEP else train_model_step,\n        evaluate_model_step,\n        data_quality_baseline_step,\n        condition_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession5_pipeline.upsert(role_arn=role)\n\nWe can now start the pipeline:\n\nsession5_pipeline.start()\n\n\n\nStep 8 - Checking Constraints and Statistics\nOur pipeline generated data baseline statistics and constraints. We can take a look at what these values look like by downloading them from S3. You need to wait for the pipeline to finish running before these files are available.\nHere are the data quality statistics:\n\nfrom sagemaker.s3 import S3Downloader\n\ntry:\n    response = json.loads(\n        S3Downloader.read_file(f\"{DATA_QUALITY_LOCATION}/statistics.json\")\n    )\n    print(json.dumps(response[\"features\"][1], indent=2))\nexcept Exception as e:\n    pass\n\n{\n  \"name\": \"island\",\n  \"inferred_type\": \"String\",\n  \"string_statistics\": {\n    \"common\": {\n      \"num_present\": 235,\n      \"num_missing\": 0\n    },\n    \"distinct_count\": 3.0,\n    \"distribution\": {\n      \"categorical\": {\n        \"buckets\": [\n          {\n            \"value\": \"Dream\",\n            \"count\": 86\n          },\n          {\n            \"value\": \"Torgersen\",\n            \"count\": 34\n          },\n          {\n            \"value\": \"Biscoe\",\n            \"count\": 115\n          }\n        ]\n      }\n    }\n  }\n}\n\n\nHere are the data quality constraints:\n\ntry:\n    response = json.loads(S3Downloader.read_file(f\"{DATA_QUALITY_LOCATION}/constraints.json\"))\n    print(json.dumps(response, indent=2))\nexcept Exception as e:\n    pass\n\n{\n  \"version\": 0.0,\n  \"features\": [\n    {\n      \"name\": \"species\",\n      \"inferred_type\": \"String\",\n      \"completeness\": 1.0,\n      \"string_constraints\": {\n        \"domains\": [\n          \"Adelie\",\n          \"Chinstrap\",\n          \"Gentoo\"\n        ]\n      }\n    },\n    {\n      \"name\": \"island\",\n      \"inferred_type\": \"String\",\n      \"completeness\": 1.0,\n      \"string_constraints\": {\n        \"domains\": [\n          \"Dream\",\n          \"Torgersen\",\n          \"Biscoe\"\n        ]\n      }\n    },\n    {\n      \"name\": \"culmen_length_mm\",\n      \"inferred_type\": \"Fractional\",\n      \"completeness\": 1.0,\n      \"num_constraints\": {\n        \"is_non_negative\": true\n      }\n    },\n    {\n      \"name\": \"culmen_depth_mm\",\n      \"inferred_type\": \"Fractional\",\n      \"completeness\": 1.0,\n      \"num_constraints\": {\n        \"is_non_negative\": true\n      }\n    },\n    {\n      \"name\": \"flipper_length_mm\",\n      \"inferred_type\": \"Fractional\",\n      \"completeness\": 1.0,\n      \"num_constraints\": {\n        \"is_non_negative\": true\n      }\n    },\n    {\n      \"name\": \"body_mass_g\",\n      \"inferred_type\": \"Fractional\",\n      \"completeness\": 1.0,\n      \"num_constraints\": {\n        \"is_non_negative\": true\n      }\n    },\n    {\n      \"name\": \"sex\",\n      \"inferred_type\": \"String\",\n      \"completeness\": 1.0,\n      \"string_constraints\": {\n        \"domains\": [\n          \"FEMALE\",\n          \"MALE\"\n        ]\n      }\n    }\n  ],\n  \"monitoring_config\": {\n    \"evaluate_constraints\": \"Enabled\",\n    \"emit_metrics\": \"Enabled\",\n    \"datatype_check_threshold\": 1.0,\n    \"domain_content_threshold\": 1.0,\n    \"distribution_constraints\": {\n      \"perform_comparison\": \"Enabled\",\n      \"comparison_threshold\": 0.1,\n      \"comparison_method\": \"Robust\",\n      \"categorical_comparison_threshold\": 0.1,\n      \"categorical_drift_method\": \"LInfinity\"\n    }\n  }\n}\n\n\nAnd here are the model quality constraints:\n\ntry:\n    response = json.loads(S3Downloader.read_file(f\"{MODEL_QUALITY_LOCATION}/constraints.json\"))\n    print(json.dumps(response, indent=2))\nexcept Exception as e:\n    pass\n\n{\n  \"version\": 0.0,\n  \"multiclass_classification_constraints\": {\n    \"accuracy\": {\n      \"threshold\": 0.88,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_recall\": {\n      \"threshold\": 0.88,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_precision\": {\n      \"threshold\": 0.8940711462450592,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_f0_5\": {\n      \"threshold\": 0.8763227513227512,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_f1\": {\n      \"threshold\": 0.8677740863787375,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_f2\": {\n      \"threshold\": 0.8722000126911604,\n      \"comparison_operator\": \"LessThanThreshold\"\n    }\n  }\n}\n\n\n\n\nStep 9 - Generating Fake Traffic\nTo test the monitoring functionality, we need to generate traffic to the endpoint. To generate traffic, we will send every sample from the dataset to the endpoint to simulate real prediction requests:\n\nfrom sagemaker.serializers import JSONSerializer\n\ndata = penguins.drop([\"species\"], axis=1)\ndata = data.dropna()\n\npredictor = Predictor(\n    endpoint_name=ENDPOINT,\n    serializer=JSONSerializer(),\n    sagemaker_session=sagemaker_session,\n)\n\nfor index, row in data.iterrows():\n    try:\n        predictor.predict(row.to_dict(), inference_id=str(index))\n    except Exception as e:\n        print(e)\n        break\n\nWe can check the location where the endpoint stores the captured data, download a file, and display its content. It may take a few minutes for the first few files to show up in S3.\nThese files contain the data captured by the endpoint in a SageMaker-specific JSON-line format. Each inference request is captured in a single line in the jsonl file. The line contains both the input and output merged together:\n\nfiles = S3Downloader.list(DATA_CAPTURE_DESTINATION)[:3]\nif len(files):\n    lines = S3Downloader.read_file(files[0])\n    print(json.dumps(json.loads(lines.split(\"\\n\")[0]), indent=2))\n\n{\n  \"captureData\": {\n    \"endpointInput\": {\n      \"observedContentType\": \"application/json\",\n      \"mode\": \"INPUT\",\n      \"data\": \"{\\\"island\\\": \\\"Torgersen\\\", \\\"culmen_length_mm\\\": 39.1, \\\"culmen_depth_mm\\\": 18.7, \\\"flipper_length_mm\\\": 181.0, \\\"body_mass_g\\\": 3750.0, \\\"sex\\\": \\\"MALE\\\"}\",\n      \"encoding\": \"JSON\"\n    },\n    \"endpointOutput\": {\n      \"observedContentType\": \"application/json\",\n      \"mode\": \"OUTPUT\",\n      \"data\": \"{\\\"prediction\\\": \\\"Adelie\\\", \\\"confidence\\\": 0.962092221}\",\n      \"encoding\": \"JSON\"\n    }\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"952c2752-29a4-4c45-b4f3-02298278ec65\",\n    \"inferenceId\": \"0\",\n    \"inferenceTime\": \"2023-11-09T22:21:44Z\"\n  },\n  \"eventVersion\": \"0\"\n}\n\n\nThese files contain the data captured by the endpoint in a SageMaker-specific JSON-line format. Each inference request is captured in a single line in the jsonl file. The line contains both the input and output merged together:\n\n\nStep 10 - Generating Fake Labels\nTo test the performance of the model, we need to label the samples captured by the endpoint. We can simulate the labeling process by generating a random label for every sample. Check Ingest Ground Truth Labels and Merge Them With Predictions for more information about this.\n\nimport random\nfrom datetime import datetime\nfrom sagemaker.s3 import S3Uploader\n\nrecords = []\nfor inference_id in range(len(data)):\n    random.seed(inference_id)\n\n    records.append(json.dumps({\n        \"groundTruthData\": {\n            \"data\": random.choice([\"Adelie\", \"Chinstrap\", \"Gentoo\"]),\n            \"encoding\": \"CSV\",\n        },\n        \"eventMetadata\": {\n            \"eventId\": str(inference_id),\n        },\n        \"eventVersion\": \"0\",\n    }))\n\ngroundtruth_payload = \"\\n\".join(records)\nupload_time = datetime.utcnow()\nuri = f\"{GROUND_TRUTH_LOCATION}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\nS3Uploader.upload_string_as_file_body(groundtruth_payload, uri)\n\n\n\nStep 11 - Preparing Monitoring Functions\nLet’s create a few functions that will help us work with monitoring schedules later on:\n\nfrom time import sleep\nfrom sagemaker.model_monitor import MonitoringExecution\n\n\ndef describe_monitoring_schedules(endpoint_name):\n    schedules = []\n    response = sagemaker_client.list_monitoring_schedules(EndpointName=endpoint_name)[\n        \"MonitoringScheduleSummaries\"\n    ]\n    for item in response:\n        name = item[\"MonitoringScheduleName\"]\n        schedule = {\n            \"Name\": name,\n            \"Type\": item[\"MonitoringType\"],\n        }\n\n        description = sagemaker_client.describe_monitoring_schedule(\n            MonitoringScheduleName=name\n        )\n\n        schedule[\"Status\"] = description[\"MonitoringScheduleStatus\"]\n\n        last_execution_status = description[\"LastMonitoringExecutionSummary\"][\n            \"MonitoringExecutionStatus\"\n        ]\n\n        schedule[\"Last Execution Status\"] = last_execution_status\n        schedule[\"Last Execution Date\"] = str(description[\"LastMonitoringExecutionSummary\"][\"LastModifiedTime\"])\n\n        if last_execution_status == \"Failed\":\n            schedule[\"Failure Reason\"] = description[\"LastMonitoringExecutionSummary\"][\n                \"FailureReason\"\n            ]\n        elif last_execution_status == \"CompletedWithViolations\":\n            processing_job_arn = description[\"LastMonitoringExecutionSummary\"][\n                \"ProcessingJobArn\"\n            ]\n            execution = MonitoringExecution.from_processing_arn(\n                sagemaker_session=sagemaker_session,\n                processing_job_arn=processing_job_arn,\n            )\n            execution_destination = execution.output.destination\n\n            violations_filepath = os.path.join(\n                execution_destination, \"constraint_violations.json\"\n            )\n            violations = json.loads(S3Downloader.read_file(violations_filepath))[\n                \"violations\"\n            ]\n\n            schedule[\"Violations\"] = violations\n\n        schedules.append(schedule)\n\n    return schedules\n\n\ndef describe_monitoring_schedule(endpoint_name, monitoring_type):\n    found = False\n\n    schedules = describe_monitoring_schedules(endpoint_name)\n    for schedule in schedules:\n        if schedule[\"Type\"] == monitoring_type:\n            found = True\n            print(json.dumps(schedule, indent=2))\n\n    if not found:\n        print(f\"There's no {monitoring_type} Monitoring Schedule.\")\n\n\ndef describe_data_monitoring_schedule(endpoint_name):\n    describe_monitoring_schedule(endpoint_name, \"DataQuality\")\n\n\ndef describe_model_monitoring_schedule(endpoint_name):\n    describe_monitoring_schedule(endpoint_name, \"ModelQuality\")\n\n\ndef delete_monitoring_schedule(endpoint_name, monitoring_type):\n    attempts = 30\n    found = False\n\n    response = sagemaker_client.list_monitoring_schedules(EndpointName=endpoint_name)[\n        \"MonitoringScheduleSummaries\"\n    ]\n    for item in response:\n        if item[\"MonitoringType\"] == monitoring_type:\n            found = True\n            \n            summary = sagemaker_client.describe_monitoring_schedule(\n                MonitoringScheduleName=item[\"MonitoringScheduleName\"]\n            )\n            status = summary[\"MonitoringScheduleStatus\"]\n\n            if status == \"Scheduled\" and \"LastMonitoringExecutionSummary\" in summary and \"MonitoringExecutionStatus\" in summary[\"LastMonitoringExecutionSummary\"]:\n                status = summary[\"LastMonitoringExecutionSummary\"][\"MonitoringExecutionStatus\"]\n\n            while status in (\"Pending\", \"InProgress\") and attempts &gt; 0:\n                attempts -= 1\n                print(\n                    f\"Monitoring schedule status: {status}. Waiting for it to finish.\"\n                )\n                sleep(30)\n\n                status = sagemaker_client.describe_monitoring_schedule(\n                    MonitoringScheduleName=item[\"MonitoringScheduleName\"]\n                )[\"MonitoringScheduleStatus\"]\n\n            if status not in (\"Pending\", \"InProgress\"):\n                sagemaker_client.delete_monitoring_schedule(\n                    MonitoringScheduleName=item[\"MonitoringScheduleName\"]\n                )\n                print(\"Monitoring schedule deleted.\")\n            else:\n                print(\"Waiting for monitoring schedule timed out\")\n\n    if not found:\n        print(f\"There's no {monitoring_type} Monitoring Schedule.\")\n\n\ndef delete_data_monitoring_schedule(endpoint_name):\n    delete_monitoring_schedule(endpoint_name, \"DataQuality\")\n\n\ndef delete_model_monitoring_schedule(endpoint_name):\n    delete_monitoring_schedule(endpoint_name, \"ModelQuality\")\n\n\n\nStep 12 - Setting Up Data Monitoring Job\nSageMaker looks for violations in the data captured by the endpoint. By default, it combines the input data with the endpoint output and compares the result with the baseline we generated. If we let SageMaker do this, we will get a few violations, for example an “extra column check” violation because the field confidence doesn’t exist in the baseline data.\nWe can fix these violations by creating a preprocessing script configuring the data we want the monitoring job to use. Check Preprocessing and Postprocessing for more information about how to configure these scripts.\nLet’s define the name of the preprocessing script:\n\nDATA_QUALITY_PREPROCESSOR = \"data_quality_preprocessor.py\"\n\nWe can now define the preprocessing script. Notice that this script will return the input data the endpoint receives with a new species column containing the prediction of the model:\n\nimport json\n\ndef preprocess_handler(inference_record):\n    input_data = json.loads(inference_record.endpoint_input.data)\n    output_data = json.loads(inference_record.endpoint_output.data)\n    \n    response = input_data\n    response[\"species\"] = output_data[\"prediction\"]\n\n    # The `response` variable contains the data that we want the\n    # monitoring job to use to compare with the baseline.\n    return response\n\nThe monitoring schedule expects an S3 location pointing to the preprocessing script. Let’s upload the script to the default bucket.\n\nbucket = boto3.Session().resource(\"s3\").Bucket(config[\"session\"].default_bucket())\nprefix = \"penguins/monitoring\"\nbucket.Object(os.path.join(prefix, DATA_QUALITY_PREPROCESSOR)).upload_file(\n    str(CODE_FOLDER / DATA_QUALITY_PREPROCESSOR)\n)\ndata_quality_preprocessor = (\n    f\"s3://{os.path.join(bucket.name, prefix, DATA_QUALITY_PREPROCESSOR)}\"\n)\n\nWe can now set up the Data Quality Monitoring Job using the DefaultModelMonitor class. Notice how we specify the record_preprocessor_script using the S3 location where we uploaded our script.\n\nfrom sagemaker.model_monitor import CronExpressionGenerator, DefaultModelMonitor\n\ndata_monitor = DefaultModelMonitor(\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    max_runtime_in_seconds=3600,\n    role=role,\n)\n\ndata_monitor.create_monitoring_schedule(\n    monitor_schedule_name=\"penguins-data-monitoring-schedule\",\n    endpoint_input=ENDPOINT,\n    record_preprocessor_script=data_quality_preprocessor,\n    statistics=f\"{DATA_QUALITY_LOCATION}/statistics.json\",\n    constraints=f\"{DATA_QUALITY_LOCATION}/constraints.json\",\n    schedule_cron_expression=CronExpressionGenerator.hourly(),\n    output_s3_uri=DATA_QUALITY_LOCATION,\n    enable_cloudwatch_metrics=True,\n)\n\nWe can check the results of the monitoring job by looking at whether it generated any violations:\n\ndescribe_data_monitoring_schedule(ENDPOINT)\n\n{\n  \"Name\": \"penguins-data-monitoring-schedule\",\n  \"Type\": \"DataQuality\",\n  \"Status\": \"Scheduled\",\n  \"Last Execution Status\": \"Failed\",\n  \"Last Execution Date\": \"2023-11-10 15:01:08.656000-05:00\",\n  \"Failure Reason\": \"Job inputs had no data\"\n}\n\n\n\n\nStep 13 - Setting up Model Monitoring Job\nTo set up a Model Quality Monitoring Job, we can use the ModelQualityMonitor class. The EndpointInput instance configures the attribute the monitoring job should use to determine the prediction from the model.\nCheck Amazon SageMaker Model Quality Monitor for a complete tutorial on how to run a Model Monitoring Job in SageMaker.\nWe can now start the Model Quality Monitoring Job:\n\nfrom sagemaker.model_monitor import ModelQualityMonitor, EndpointInput\n\nmodel_monitor = ModelQualityMonitor(\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    max_runtime_in_seconds=1800,\n    role=role\n)\n\nmodel_monitor.create_monitoring_schedule(\n    monitor_schedule_name=\"penguins-model-monitoring-schedule\",\n    \n    endpoint_input = EndpointInput(\n        endpoint_name=ENDPOINT,\n        inference_attribute=\"prediction\",\n        destination=\"/opt/ml/processing/input_data\",\n    ),\n    \n    problem_type=\"MulticlassClassification\",\n    ground_truth_input=GROUND_TRUTH_LOCATION,\n    constraints=f\"{MODEL_QUALITY_LOCATION}/constraints.json\",\n    schedule_cron_expression=CronExpressionGenerator.hourly(),\n    output_s3_uri=MODEL_QUALITY_LOCATION,\n    enable_cloudwatch_metrics=True,\n)\n\nWe can check the results of the monitoring job by looking at whether it generated any violations.\nHere is an example of a few violations generated by the monitoring job:\n{\n  \"Name\": \"penguins-model-monitoring-schedule\",\n  \"Type\": \"ModelQuality\",\n  \"Status\": \"Scheduled\",\n  \"Last Execution Status\": \"CompletedWithViolations\",\n  \"Last Execution Date\": \"2023-11-10 07:15:38.870000-05:00\",\n  \"Violations\": [\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedF2 with 0.35599535646931135 +/- 0.014588104295968964 was LessThanThreshold '0.8722000126911604'\",\n      \"metric_name\": \"weightedF2\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric accuracy with 0.3691860465116279 +/- 0.01318589816003266 was LessThanThreshold '0.88'\",\n      \"metric_name\": \"accuracy\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedRecall with 0.3691860465116279 +/- 0.013185898160032667 was LessThanThreshold '0.88'\",\n      \"metric_name\": \"weightedRecall\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedPrecision with 0.3723930543404764 +/- 0.026898711106593986 was LessThanThreshold '0.8940711462450592'\",\n      \"metric_name\": \"weightedPrecision\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedF1 with 0.34565734045211405 +/- 0.01766811466802452 was LessThanThreshold '0.8677740863787375'\",\n      \"metric_name\": \"weightedF1\"\n    }\n  ]\n}\n\ndescribe_model_monitoring_schedule(ENDPOINT)\n\n{\n  \"Name\": \"penguins-model-monitoring-schedule\",\n  \"Type\": \"ModelQuality\",\n  \"Status\": \"Scheduled\",\n  \"Last Execution Status\": \"CompletedWithViolations\",\n  \"Last Execution Date\": \"2023-11-10 07:15:38.870000-05:00\",\n  \"Violations\": [\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedF2 with 0.35599535646931135 +/- 0.014588104295968964 was LessThanThreshold '0.8722000126911604'\",\n      \"metric_name\": \"weightedF2\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric accuracy with 0.3691860465116279 +/- 0.01318589816003266 was LessThanThreshold '0.88'\",\n      \"metric_name\": \"accuracy\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedRecall with 0.3691860465116279 +/- 0.013185898160032667 was LessThanThreshold '0.88'\",\n      \"metric_name\": \"weightedRecall\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedPrecision with 0.3723930543404764 +/- 0.026898711106593986 was LessThanThreshold '0.8940711462450592'\",\n      \"metric_name\": \"weightedPrecision\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedF1 with 0.34565734045211405 +/- 0.01766811466802452 was LessThanThreshold '0.8677740863787375'\",\n      \"metric_name\": \"weightedF1\"\n    }\n  ]\n}\n\n\n\n\nStep 14 - Tearing Down Resources\nThe following code will stop the monitoring jobs and delete the endpoint.\n\ndelete_data_monitoring_schedule(ENDPOINT)\ndelete_model_monitoring_schedule(ENDPOINT)\n\nLet’s delete the endpoint:\n\npredictor.delete_endpoint()\n\n\n\nAssignments\n\nAssignment 5.1 You can visualize the results of your monitoring jobs in Amazon SageMaker Studio. Go to your endpoint, and visit the Data quality and Model quality tabs. View the details of your monitoring jobs, and create a few charts to explore the baseline and the captured values for any metric that the monitoring job calculates.\nAssignment 5.2 The QualityCheck Step runs a processing job to compute baseline statistics and constraints from the input dataset. We configured the pipeline to generate the initial baselines every time it runs. Modify the code to prevent the pipeline from registering a new version of the model if the dataset violates the baseline of the previous model version. You can configure the QualityCheck Step to accomplish this.\nAssignment 5.3 We are generating predictions for the test set twice during the execution of our pipeline. First, during the Evaluation Step, and then using a Transform Step in anticipation of generating the baseline to monitor the model. Modify the Evaluation Step so it reuses the model performance computed by the QualityCheck Step instead of generating predictions again.\nAssignment 5.4 Evidently AI is an open-source Machine Learning observability platform that you can use to evaluate, test, and monitor models. For this assignment, integrate the endpoint we built with Evidently AI to use its capabilities to monitor the model.\nAssignment 5.5 Instead of running the entire pipeline from start to finish, sometimes you may only need to iterate over particular steps. SageMaker Pipelines supports Selective Execution for Pipeline Steps. In this assignment you will use Selective Execution to only run one specific step of the pipeline. Unlocking efficiency: Harnessing the power of Selective Execution in Amazon SageMaker Pipelines is a great article that explains this feature."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to the program!"
  },
  {
    "objectID": "index.html#program-structure",
    "href": "index.html#program-structure",
    "title": "Introduction",
    "section": "Program Structure",
    "text": "Program Structure\n\nSession 1 - Production Machine Learning Is Different\n\nAn overview of the components of a machine learning system\nThe role of data in real-world applications\n4 popular sampling strategies to collect data\nLabeling strategies and a quick introduction to weak supervision and active learning\nData splitting and how data leakage can destroy your models\nBuilding good features and a quick introduction to imputation, standardization, and encoding\nProcessing data at scale using data parallelism\nUsing pipelines to orchestrate machine learning workflows\nA template architecture of a production-ready machine learning system\nUnderstanding SageMaker’s Processing Step and Processing Jobs\n\n\n\nSession 2 - Building Models And The Training Pipeline\n\nThe first rule of Machine Learning Engineering\nA 3-step process to solve a problem using machine learning\n9 tips to select the best machine learning model for your solution\nStrategies for working with imbalanced data, dealing with rare events, and a quick introduction to cost-sensitive learning\nThe reason you should not balance your data\nAn introduction to hyperparameter tuning\nThe importance of reproducibility and a quick introduction to experiment tracking\nDistributed Training using data and model parallelism\nUnderstanding SageMaker’s Training and Tuning Steps, and Training and Tuning Jobs\n\n\n\nSession 3 - Evaluating and Versioning Models\n\nThe difference between good models and useful models\nFraming evaluation metrics in the context of business performance\nAn 8-step process to evaluate machine learning models\nIntroduction to backtesting\nHow to deal with disproportionally important examples and rare cases\nStrategies to determine whether a model is fair and robust to future changes\nA 3-step process to perform error analysis and measure the impact of potential improvements\nHow to determine whether individual predictions are useful\nEvaluating Large Language Models using Supervised Evaluation and Self-Evaluation\nAn introduction to model versioning\nUnderstanding SageMaker’s Model Registry, Condition, and Model Steps\n\n\n\nSession 4 - Deploying Models and Serving Predictions\n\nHow do model performance, speed, and cost affect models in production\nLatency, throughput, and their relationships\nUnderstanding on-demand inference and batch inference and when to use each one\nHow to make models run fast using model compression and a quick introduction to quantization and knowledge distillation\nDeploying models in dedicated and multi-model endpoints\nA comparison of the tools you can use to serve predictions\nDesigning a 3-component inference pipeline\nUnderstanding the internal structure of a SageMaker Endpoint\nUnderstanding SageMaker’s PipelineModel and Amazon EventBridge\n\n\n\nSession 5 - Data Distribution Shifts And Model Monitoring\n\nThe 3 most common problems your model will face in production\nAn introduction to data distribution shifts, edge cases, and unintended feedback loops\nCatastrophic predictions and the problem with edge cases\nUnderstanding covariate shift and concept drift\nMonitoring schema violations, data statistics, model performance, prediction distribution, and changes in user feedback\nThe 3 strategies to keep your models working despite data distribution shifts\nUnderstanding SageMaker’s Transform Step, QualityCheck Step, Transform Jobs, and Monitoring Jobs\n\n\n\nSession 6 - Continual Learning And Testing in Production\n\nThe importance of Continual Learning and why every company wants to to do it\n3 challenges when implementing Continual Learning\nA 4-step plan to implement Continual Learning\nHow to determine what data to use to retrain a model\nA 3-step progressive plan to decide how frequently you should retrain your models\nThe differences between training from scratch and incremental training\nAn introduction to Testing in Production\n5 strategies to test models in production: A/B testing, shadow deployments, canary releases, interleaving experiments, and multi-armed bandits\nHighlights from the program"
  },
  {
    "objectID": "teardown.html",
    "href": "teardown.html",
    "title": "Tear Down",
    "section": "",
    "text": "import sys\nfrom pathlib import Path\n\nCODE_FOLDER = Path(\"code\")\nsys.path.append(f\"./{CODE_FOLDER}\")\nimport boto3\n\nfrom constants import *\nfrom sagemaker.experiments.experiment import Experiment"
  },
  {
    "objectID": "teardown.html#remove-experiments",
    "href": "teardown.html#remove-experiments",
    "title": "Tear Down",
    "section": "Remove Experiments",
    "text": "Remove Experiments\nTo avoid incurring unnecessary charges, delete the SageMaker Experiment resources you no longer need.\n\nEXPERIMENT_NAME = \"tensorfl-ap2uhikoiith-KnMow1gYgw-aws-tuning-job\"\n\nexperiment = Experiment.load(experiment_name=EXPERIMENT_NAME, sagemaker_session=sagemaker_session)\nexperiment._delete_all(action=\"--force\")"
  },
  {
    "objectID": "teardown.html#clean-up-model-registry",
    "href": "teardown.html#clean-up-model-registry",
    "title": "Tear Down",
    "section": "Clean up Model Registry",
    "text": "Clean up Model Registry\nThis section deletes every model registered under a specific model group and then it removes the model group.\n\nfor mp in sagemaker_client.list_model_packages(ModelPackageGroupName=MODEL_PACKAGE_GROUP)[\"ModelPackageSummaryList\"]:\n    print(f\"Deleting {mp['ModelPackageArn']}\")\n    sagemaker_client.delete_model_package(ModelPackageName=mp[\"ModelPackageArn\"])\n\nsagemaker_client.delete_model_package_group(ModelPackageGroupName=MODEL_PACKAGE_GROUP)\n\nDeleting arn:aws:sagemaker:us-east-1:325223348818:model-package/emocional/1\n\n\n{'ResponseMetadata': {'RequestId': '3ee0e924-ab64-46cb-ada0-80ea8e5764a4',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '3ee0e924-ab64-46cb-ada0-80ea8e5764a4',\n   'content-type': 'application/x-amz-json-1.1',\n   'content-length': '0',\n   'date': 'Mon, 14 Aug 2023 18:18:20 GMT'},\n  'RetryAttempts': 0}}"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup Instructions",
    "section": "",
    "text": "Here are the steps you need to follow to set up the project:\nStart by forking the program’s GitHub Repository and clone it on your local computer.\nCreate and activate a virtual environment:\nOnce the virtual environment is active, you can update pip and install the libraries in the requirements.txt file:\nWe’ll use Jupyter Notebooks during the program. Using the following command, you can install a Jupyter kernel in the virtual environment. If you use Visual Studio Code, you can point your kernel to the virtual environment, and it will install it automatically:\nInstall Docker. You’ll find installation instructions on their site for your particular environment. After you install it, you can verify Docker is running using the following command:\nAt this point you can open the project using Visual Studio Code or your favorite IDE. Make sure you point the Jupyter kernel to the virtual environment that you created before."
  },
  {
    "objectID": "setup.html#configuring-aws",
    "href": "setup.html#configuring-aws",
    "title": "Setup Instructions",
    "section": "Configuring AWS",
    "text": "Configuring AWS\nIf you don’t have one yet, create a new AWS account. A community member noticed that indicating the account is for personal use and his interest is in Machine Learning gave him immediate access to the hardware we need for the program.\nWe’ll need access to ml.m5.xlarge instances. By default, the quota for a new account is zero, but the tip above might fix this problem. If it doesn’t, you’ll need to request a quota increase.\nYou can do this in your AWS account under Service Quotas &gt; AWS Services &gt; Amazon SageMaker. Find ml.m5.xlarge and request a quota increase for processing jobs, training jobs, transform jobs, and endpoint usage. Ask for a minimum of 3 instances.\nYou’ll need access to AWS from your local environment. Install the AWS CLI and configure it with your aws_access_key_id and aws_secret_access_key.\nTo get an access key, you first need to open the IAM service, find your user, select Security Credentials, then assign a Multi-Factor Authentication (MFA) device and follow the prompts. After setup and verified, you can click to create an access key.\nAfter you finish configuring the CLI, create a new S3 bucket where we will store the data and every resource we are going to create during the program. The name of the bucket must be unique:\n$ aws s3api create-bucket --bucket [YOUR-BUCKET-NAME]\n\n\n\n\n\n\nNote\n\n\n\nIf you want to create a bucket in a region other than us-east-1, you need to use the --create-bucket-configuration argument to specify your LocationConstraint. See the example below:\n\n\n$ aws s3api create-bucket --bucket [YOUR-BUCKET-NAME] \\\n    --create-bucket-configuration LocationConstraint=\"eu-west-1\"\nUpload the dataset to the S3 bucket you just created:\n$ aws s3 cp program/penguins.csv s3://[YOUR-BUCKET-NAME]/penguins/data/data.csv"
  },
  {
    "objectID": "setup.html#configuring-sagemaker",
    "href": "setup.html#configuring-sagemaker",
    "title": "Setup Instructions",
    "section": "Configuring SageMaker",
    "text": "Configuring SageMaker\nIf you don’t have one yet, create a SageMaker domain. The Getting Started on Amazon SageMaker Studio video will walk you through the process.\nAfter you are done, run the following command to return the Domain Id and the User Profile Name of your SageMaker domain:\n$ aws sagemaker list-user-profiles | grep -E '\"DomainId\"|\"UserProfileName\"' \\\n    | awk -F'[:,\"]+' '{print $2\":\"$3 $4 $5}'\nUse the DomainId and the UserProfileName from the response and replace them in the following command that we’ll return the execution role attached to the user:\n$ aws sagemaker describe-user-profile \\\n    --domain-id [YOUR-DOMAIN-ID] \\\n    --user-profile-name [YOUR-USER-PROFILE-NAME] \\\n    | grep -E \"ExecutionRole\" | awk -F'[\"]' '{print $2\": \"$4}'\nCreate an .env file in the root directory of your repository with the following content. Make sure you replace the value of each variable with the correct value:\nBUCKET=[YOUR-BUCKET-NAME]\nDOMAIN_ID=[YOUR-DOMAIN-ID]\nUSER_PROFILE=[YOUR-USER-PROFILE]\nROLE=[YOUR-EXECUTION-ROLE]\nOpen the Amazon IAM service, find the Execution Role from before and edit the custom Execution Policy assigned to it. Edit the permissions of the Execution Policy and replace them with the JSON below. These permissions will give the Execution Role access to the resources we’ll use during the program:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"IAM0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:CreateServiceLinkedRole\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:AWSServiceName\": [\n                        \"autoscaling.amazonaws.com\",\n                        \"ec2scheduled.amazonaws.com\",\n                        \"elasticloadbalancing.amazonaws.com\",\n                        \"spot.amazonaws.com\",\n                        \"spotfleet.amazonaws.com\",\n                        \"transitgateway.amazonaws.com\"\n                    ]\n                }\n            }\n        },\n        {\n            \"Sid\": \"IAM1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:CreateRole\",\n                \"iam:DeleteRole\",\n                \"iam:PassRole\",\n                \"iam:AttachRolePolicy\",\n                \"iam:DetachRolePolicy\",\n                \"iam:CreatePolicy\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"Lambda\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:CreateFunction\",\n                \"lambda:DeleteFunction\",\n                \"lambda:InvokeFunctionUrl\",\n                \"lambda:InvokeFunction\",\n                \"lambda:UpdateFunctionCode\",\n                \"lambda:InvokeAsync\",\n                \"lambda:AddPermission\",\n                \"lambda:RemovePermission\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"SageMaker\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:UpdateDomain\",\n                \"sagemaker:UpdateUserProfile\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"CloudWatch\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"cloudwatch:PutMetricData\",\n                \"cloudwatch:GetMetricData\",\n                \"cloudwatch:DescribeAlarmsForMetric\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:CreateLogGroup\",\n                \"logs:DescribeLogStreams\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"ECR\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:BatchGetImage\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"S3\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:CreateBucket\",\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": \"arn:aws:s3:::*\"\n        },\n        {\n            \"Sid\": \"EventBridge\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"events:PutRule\",\n                \"events:PutTargets\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\nFinally, find the Trust relationships section under the same Execution Role, edit the configuration, and replace it with the JSON below:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": [\n                    \"sagemaker.amazonaws.com\", \n                    \"events.amazonaws.com\"\n                ]\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}"
  },
  {
    "objectID": "setup.html#apple-silicon",
    "href": "setup.html#apple-silicon",
    "title": "Setup Instructions",
    "section": "Apple silicon",
    "text": "Apple silicon\nIf your local environment is running on Apple silicon, you need to build a TensorFlow docker image to run some of the pipeline steps on your local computer. This is because SageMaker doesn’t provide out-of-the-box TensorFlow images compatible with Apple silicon.\nYou can build the image running the following command:\n$ docker build -t sagemaker-tensorflow-toolkit-local container/.\nAfter building this Docker image, the notebook will automatically use it when running the pipeline in Local Mode on your Mac. There’s nothing else you need to do."
  },
  {
    "objectID": "setup.html#running-the-code-in-sagemaker-studio",
    "href": "setup.html#running-the-code-in-sagemaker-studio",
    "title": "Setup Instructions",
    "section": "Running the code in SageMaker Studio",
    "text": "Running the code in SageMaker Studio\nIf you are planning to run the code from inside SageMaker Studio, you will need to create a Lifecycle Configuration to update the kernel. To do this, you need to run the studio-setup.ipynb notebook once inside SageMaker Studio. After doing this, you can use the TensorFlow 2.11 Python 3.9 CPU Optimized kernel with the start-up script named ml-school."
  }
]